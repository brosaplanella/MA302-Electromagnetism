[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA302 Electromagnetism",
    "section": "",
    "text": "Preface\nThese are the lecture notes for the third-year Maths module MA302 Electromagnetism taught at the University of Warwick. These notes are available both as a PDF and a static website (which should be suitable for screen-reading devices), you can access both at https://brosaplanella.github.io/MA302-Electromagnetism/. I might update the notes as we go, fixing typos and improving explanations. You can keep track of those changes in the CHANGELOG. Further material is available on Moodle for registered students.\nThese lectures notes, which aim to be self–contained, are inspired by three main sources:\nThe first two are good references if you want to read more about the topic, probably Tong’s book is closer in structure to these lecture notes. In both cases, they cover a lot more material than this module does.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#aims-and-structure",
    "href": "index.html#aims-and-structure",
    "title": "MA302 Electromagnetism",
    "section": "Aims and structure",
    "text": "Aims and structure\nThe main aims of this module are:\n\nProvide the student with the background necessary to understand basic electromagnetism concepts and Maxwell’s equations.\nApply this knowledge to write and solve models for simple electromagnetism setups.\nHighlight the connections of electromagnetism to practical applications in our day-to-day lives.\n\nWe will look at electromagnetism from a mathematical perspective, and use it to better understand the world around us, which is what applied mathematics is about.\nIn 1  Introduction to electromagnetism we will provide some motivation to the topic and recap some basic results from vector calculus that we will use in this module.\nWe will start with electrostatics in 2  Electrostatics, which is the study of static electric charges. We will introduce some fundamental concepts like electric charge, electric field and electrostatic potential; and derive some important results like Gauss’ law. In 3  Applications of electrostatics we will put into practice what we learned about electrostatics, by developing some concepts further and using them to understand some real applications, like conductors or capacitors.\nNext, we will turn our attention to magnetostatics (4  Magnetostatics), which is the study of steady magnetic fields. As magnetic fields are produced by charges in motion, we will introduce the concepts of current and current density. We will also derive some key results, like the Biot-Savart law, Gauss’ law for magnetism or Ampère’s law. Similarly to electrostatics, in 5  Applications of magnetostatics we will focus on applications of magnetostatics.\nIn 6  Electrodynamics we will bring time into the equation, and generalise our previous results to allow for time-dependence, leading to the Maxwell’s equations. We will talk about induction and displacement currents, but the spotlight will be on light.1 We will derive the governing equation for electromagnetic waves, and introduce some basic results, though if you want to learn a lot more about wave you should probably sign up for MA301 Waves and Metamaterials.\nWe will conclude with 7  Electromagnetism in matter, in which we will extend the Maxwell’s equations to account for real macroscale materials, rather than just charges in motion, and explain some of the phenomena that arise.\nI would like to thank Dr Kawa Manmi for his usual feedback on these notes and the rest of the material of the module. Still, this is the first time I teach this module, and there will doubtlessly be errors and typos in these notes. There are also probably sections that could be better explained. If you spot anything or you have any suggestions, please do let me know via email.\nDr Ferran Brosa Planella, Autumn 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "MA302 Electromagnetism",
    "section": "",
    "text": "Pun intended.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1-introduction.html",
    "href": "1-introduction.html",
    "title": "1  Introduction to electromagnetism",
    "section": "",
    "text": "1.1 Recap on vector calculus\nElectromagnetism governs the interactions between particles that carry electric charge. It is one of the four fundamental forces, alongside gravity and the two nuclear forces (weak and strong). At the atomic scale, electromagnetism dictates how atoms and molecules interact, which in turn determines the many properties that materials exhibit. This means that electromagnetism underpins all of chemistry and, by extension, biology.\nThis has consequences that extend far beyond the microscopic world. Many of the physical phenomena we encounter in everyday life arise from electromagnetic interactions: friction, electricity, magnetism, and electromagnetic radiation such as microwaves, X-rays, and visible light. In fact, apart from gravity and its effects, almost everything else we experience daily is a consequence of electromagnetism.\nWhat makes electromagnetism remarkable is that, despite giving rise to such a wide range of phenomena, it can be described by just four elegant equations: the Maxwell’s equations:\n\\nabla \\cdot {\\boldsymbol{E}}= \\frac{\\rho}{\\epsilon_0}, \\tag{1.1} \\nabla \\cdot {\\boldsymbol{B}}= 0, \\tag{1.2} \\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial {\\boldsymbol{B}}}{\\partial t}, \\tag{1.3} \\nabla \\times {\\boldsymbol{B}}= \\mu_0 \\left( {\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\right). \\tag{1.4}\nThese equations govern the behaviour of the electric field {\\boldsymbol{E}} and the magnetic field {\\boldsymbol{B}}, and from them we can explain all the phenomena mentioned above. From a mathematical perspective, electromagnetism is fascinating because it applies the vector calculus concepts you have learned in previous modules and demonstrates their power. The goal of this module is to derive Maxwell’s equations and use them to explain real-world phenomena.\nBefore going into electromagnetism, let’s review some notation and basic results from vector calculus, that you should already be familiar with from previous modules. Throughout all the module, all functions and vector fields are assumed to be sufficiently “nice” unless otherwise stated. This means, for example, that we take all functions to be smooth enough so all partial derivatives up to whatever order is needed exist.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to electromagnetism</span>"
    ]
  },
  {
    "objectID": "1-introduction.html#recap-on-vector-calculus",
    "href": "1-introduction.html#recap-on-vector-calculus",
    "title": "1  Introduction to electromagnetism",
    "section": "",
    "text": "1.1.1 Vectors in \\mathbb{R}^3\nIn this module we will work mostly in \\mathbb{R}^3 or a subdomain, and in Cartesian coordinates. Given the standard orthonormal base {\\boldsymbol{e}}_x = (1, 0, 0), {\\boldsymbol{e}}_y = (0, 1, 0) and {\\boldsymbol{e}}_z = (0, 0, 1), the position vector is given by\n {\\boldsymbol{r}}= x {\\boldsymbol{e}}_x + y {\\boldsymbol{e}}_y + z {\\boldsymbol{e}}_z, \\tag{1.5}\nor {\\boldsymbol{r}}= (x, y, z). For convenience, we may interchange notation, using x = x_1, y = x_2 and z = x_3 (and {\\boldsymbol{e}}_i = {\\boldsymbol{e}}_{x_i}), such that Equation 1.5 can be written more compactly as\n{\\boldsymbol{r}}= \\sum_{i=1}^3 x_i {\\boldsymbol{e}}_i.\nBy default, we will use the Euclidean norm, i.e.\n|{\\boldsymbol{r}}| = \\sqrt{x^2 + y^2 + z^2}.\nSimilarly, we can write a vector field {\\boldsymbol{f}}({\\boldsymbol{r}}) in terms of the orthonormal basis\n{\\boldsymbol{f}}({\\boldsymbol{r}}) = \\sum_{i=1}^3 f_i({\\boldsymbol{r}}) {\\boldsymbol{e}}_i,\nwhere f_i are the components of {\\boldsymbol{f}}.\nGiven three vectors {\\boldsymbol{a}}, {\\boldsymbol{b}} and {\\boldsymbol{c}}, we define the scalar product as\n{\\boldsymbol{a}}\\cdot {\\boldsymbol{b}}= \\sum_{i=1}^3 a_i b_i.\nNote that the Euclidean norm can be written in terms of the scalar product as |{\\boldsymbol{a}}|^2 = {\\boldsymbol{a}}\\cdot {\\boldsymbol{a}}.\nWe can also define the cross product as\n{\\boldsymbol{a}}\\times {\\boldsymbol{b}}= (a_2 b_3 - a_3 b_2) {\\boldsymbol{e}}_1 + (a_3 b_1 - a_1 b_3) {\\boldsymbol{e}}_2 + (a_1 b_2 - a_2 b_1) {\\boldsymbol{e}}_3.\nBoth products are related by the vector triple product identity, i.e.1\n{\\boldsymbol{a}}\\times ({\\boldsymbol{b}}\\times {\\boldsymbol{c}}) = ({\\boldsymbol{a}}\\cdot {\\boldsymbol{c}}) {\\boldsymbol{b}}- ({\\boldsymbol{a}}\\cdot {\\boldsymbol{b}}) {\\boldsymbol{c}}. \\tag{1.6}\n\n\n1.1.2 Vector operators\nWe now turn our attention to the vector differential operators: the gradient, the divergence and the curl.\n\nDefinition 1.1 (Gradient) The gradient of a scalar function \\psi({\\boldsymbol{r}}) is\n\\nabla \\psi = \\sum_{i=1}^3 \\frac{\\partial \\psi}{\\partial x_i} {\\boldsymbol{e}}_i = \\left(\\frac{\\partial \\psi}{\\partial x}, \\frac{\\partial \\psi}{\\partial y}, \\frac{\\partial \\psi}{\\partial z} \\right).\n\n\nDefinition 1.2 (Divergence) The divergence of a vector field {\\boldsymbol{f}}({\\boldsymbol{r}}) is\n\\nabla \\cdot {\\boldsymbol{f}}= \\sum_{i=1}^3 \\frac{\\partial f_i}{\\partial x_i} = \\frac{\\partial f_1}{\\partial x} + \\frac{\\partial f_2}{\\partial y} + \\frac{\\partial f_3}{\\partial z}.\n\n\nDefinition 1.3 (Curl) The curl of a vector field {\\boldsymbol{f}}({\\boldsymbol{r}}) is\n\\nabla \\times {\\boldsymbol{f}}= \\sum_{i=1}^3 {\\boldsymbol{e}}_i \\times \\frac{\\partial {\\boldsymbol{f}}}{\\partial x_i} = \\left(\\frac{\\partial f_3}{\\partial x_2} - \\frac{\\partial f_2}{\\partial x_3}\\right) {\\boldsymbol{e}}_1 + \\left(\\frac{\\partial f_1}{\\partial x_3} - \\frac{\\partial f_3}{\\partial x_1}\\right) {\\boldsymbol{e}}_2 + \\left(\\frac{\\partial f_2}{\\partial x_1} - \\frac{\\partial f_1}{\\partial x_2}\\right) {\\boldsymbol{e}}_3.\n\n\nDefinition 1.4 (Laplacian) The Laplacian of a scalar function \\psi({\\boldsymbol{r}}) is\n\\nabla^2 \\psi = \\sum_{i=1}^3 \\frac{\\partial^2 \\psi}{\\partial x_i^2} = \\frac{\\partial^2 \\psi}{\\partial x^2} + \\frac{\\partial^2 \\psi}{\\partial y^2} + \\frac{\\partial^2 \\psi}{\\partial z^2}.\nSimilarly, the Laplacian of vector field {\\boldsymbol{f}}({\\boldsymbol{r}}) is\n\\nabla^2 {\\boldsymbol{f}}= \\sum_{i=1}^3 \\frac{\\partial^2 {\\boldsymbol{f}}}{\\partial x_i^2} = \\frac{\\partial^2 {\\boldsymbol{f}}}{\\partial x^2} + \\frac{\\partial^2 {\\boldsymbol{f}}}{\\partial y^2} + \\frac{\\partial^2 {\\boldsymbol{f}}}{\\partial z^2}.\n\nRemember that the Laplacian can also be defined as\n\\nabla^2 \\psi = \\nabla \\cdot \\left( \\nabla \\psi \\right).\nWe can now define two very important and useful properties of these operators:\n \\nabla \\times \\left( \\nabla \\psi \\right) = {\\boldsymbol{0}}, \\tag{1.7}\nand\n\\nabla \\cdot \\left( \\nabla \\times {\\boldsymbol{f}}\\right) = 0. \\tag{1.8}\nThat is: the curl of a gradient is zero, and the divergence of a curl is zero.\nMoreover, we have that the curl of a curl can be written as\n\\nabla \\times \\left( \\nabla \\times {\\boldsymbol{f}}\\right) = \\nabla \\left( \\nabla \\cdot {\\boldsymbol{f}}\\right) - \\nabla^2 {\\boldsymbol{f}}. \\tag{1.9}\nWe can also rewrite the curl and divergence of a cross product as\n\\nabla \\times \\left( {\\boldsymbol{a}}\\times {\\boldsymbol{b}}\\right) = {\\boldsymbol{a}}\\left( \\nabla \\cdot {\\boldsymbol{b}}\\right) - {\\boldsymbol{b}}\\left( \\nabla \\cdot {\\boldsymbol{a}}\\right) + \\left( {\\boldsymbol{b}}\\cdot \\nabla \\right) {\\boldsymbol{a}}- \\left( {\\boldsymbol{a}}\\cdot \\nabla \\right) {\\boldsymbol{b}}, \\tag{1.10}\nand\n\\nabla \\cdot \\left( {\\boldsymbol{a}}\\times {\\boldsymbol{b}}\\right) = {\\boldsymbol{b}}\\cdot \\left( \\nabla \\times {\\boldsymbol{a}}\\right) - {\\boldsymbol{a}}\\cdot \\left( \\nabla \\times {\\boldsymbol{b}}\\right). \\tag{1.11}\nIt will also be handy to define the curl and divergence of the product of a scalar function and a vector field:\n\\nabla \\times \\left( \\psi {\\boldsymbol{f}}\\right) = \\left(\\nabla \\psi \\right) \\times {\\boldsymbol{f}}+ \\psi \\left( \\nabla \\times {\\boldsymbol{f}}\\right), \\tag{1.12}\nand\n\\nabla \\cdot \\left( \\psi {\\boldsymbol{f}}\\right) = \\left(\\nabla \\psi \\right) \\cdot {\\boldsymbol{f}}+ \\psi \\left( \\nabla \\cdot {\\boldsymbol{f}}\\right). \\tag{1.13}\nFinally, we need to present some identities related to {\\boldsymbol{r}} and |{\\boldsymbol{r}}|, which will be very handy in electromagnetism:\n\\nabla \\frac{1}{|{\\boldsymbol{r}}|} = - \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3}, \\tag{1.14}\nand\n\\nabla \\cdot \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} = 0, \\tag{1.15}\nfor {\\boldsymbol{r}}\\in \\mathbb{R}^3 \\setminus \\{0\\}.\n\n\n1.1.3 Integral theorems\nNow let’s talk about integrals.\n\nDefinition 1.5 (Line integral) Let C be the curve in \\mathbb{R}^3 parameterised by {\\boldsymbol{r}}(t) : [t_0, t_1] \\to \\mathbb{R}^3. Then, we can define the line integral of a scalar field \\psi and a vector field {\\boldsymbol{f}} as\n\\int_C \\psi \\; {\\mathrm{d}}s = \\int_{t_0}^{t_1} \\psi({\\boldsymbol{r}}(t)) \\left| \\frac{{\\mathrm{d}}{\\boldsymbol{r}}}{{\\mathrm{d}}t} \\right| \\; {\\mathrm{d}}t,\nand\n\\int_C {\\boldsymbol{f}}\\cdot {\\boldsymbol{t}}\\; {\\mathrm{d}}s = \\int_{t_0}^{t_1} {\\boldsymbol{f}}({\\boldsymbol{r}}(t)) \\cdot \\frac{{\\mathrm{d}}{\\boldsymbol{r}}}{{\\mathrm{d}}t} \\; {\\mathrm{d}}t,\nwhere {\\boldsymbol{t}}= \\frac{{\\mathrm{d}}{\\boldsymbol{r}}}{{\\mathrm{d}}t} is the tangent vector to the curve.\n\nWe say a curve is simple if it does not self-intersect (i.e. {\\boldsymbol{r}} is injective), and we say a curve is closed if it starts at the same point it ends (i.e. {\\boldsymbol{r}}(t_0) = {\\boldsymbol{r}}(t_1)).\n\nDefinition 1.6 (Surface integral) Let \\Sigma be the surface in \\mathbb{R}^3 parameterised by {\\boldsymbol{r}}(u,v) : D \\subseteq \\mathbb{R}^2 \\to \\mathbb{R}^3. We first define the unit normal vector to the surface {\\boldsymbol{n}} as\n{\\boldsymbol{n}}= \\frac{{\\boldsymbol{t}}_u \\times {\\boldsymbol{t}}_v}{|{\\boldsymbol{t}}_u \\times {\\boldsymbol{t}}_v|},\nwhere\n{\\boldsymbol{t}}_u = \\frac{\\partial {\\boldsymbol{r}}}{\\partial u}, \\quad \\text{and} \\quad {\\boldsymbol{t}}_v = \\frac{\\partial {\\boldsymbol{r}}}{\\partial v},\nare two tangent vectors to the surface.\nWe can define the surface integral of a scalar field \\psi and a vector field {\\boldsymbol{f}} as\n\\int_\\Sigma \\psi \\; {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\iint_D \\psi({\\boldsymbol{r}}(u,v)) \\left| \\frac{\\partial {\\boldsymbol{r}}}{\\partial u} \\times \\frac{\\partial {\\boldsymbol{r}}}{\\partial v} \\right| {\\mathrm{d}}u \\; {\\mathrm{d}}v,\nand\n \\int_\\Sigma {\\boldsymbol{f}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\iint_D {\\boldsymbol{f}}({\\boldsymbol{r}}(u,v)) \\cdot \\left( \\frac{\\partial {\\boldsymbol{r}}}{\\partial u} \\times \\frac{\\partial {\\boldsymbol{r}}}{\\partial v} \\right) {\\mathrm{d}}u \\; {\\mathrm{d}}v.\nNote that quite often it will not be possible to parameterise a whole surface using a single domain D. In these cases, we can split the surface into multiple parts, each one parameterised in a given domain D_i.\n\nWe say a surface \\Sigma is orientable if there is a choice of continuous unit normal vector field {\\boldsymbol{n}} on the surface.2 If an orientable surface \\Sigma has boundary \\partial \\Sigma (a simple closed curve) then the normal {\\boldsymbol{n}} induces an orientation on \\partial \\Sigma: we impose that {\\boldsymbol{t}}\\times {\\boldsymbol{n}} must point away from \\Sigma. Therefore, the choice of either {\\boldsymbol{t}} or {\\boldsymbol{n}} determines the sign of the other one.\nNow we can introduce two very important theorems.\n\nTheorem 1.1 (Stokes’ theorem) Let \\Sigma be a smooth orientable surface in \\mathbb{R}^3, and {\\boldsymbol{f}} a smooth vector field. Then\n\\int_\\Sigma \\left(\\nabla \\times {\\boldsymbol{f}}\\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_{\\partial \\Sigma} {\\boldsymbol{f}}\\cdot {\\boldsymbol{t}}\\; {\\mathrm{d}}s.\n\n\nTheorem 1.2 (Divergence theorem) Let \\Omega be a bounded region of \\mathbb{R}^3, and {\\boldsymbol{f}} a smooth vector field. Then\n\\int_\\Omega \\nabla \\cdot {\\boldsymbol{f}}\\; {\\mathrm{d}}V = \\int_{\\partial \\Omega} {\\boldsymbol{f}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A,\nwhere {\\boldsymbol{n}} is the normal unit vector to \\partial \\Omega pointing outwards.\n\n\n\n1.1.4 Other coordinate systems\nIn this module we will mostly work in Cartesian coordinates, but occasionally it might be more convenient to use cylindrical or spherical coordinates. In this section we provide a brief overview of them\n\n1.1.4.1 Cylindrical coordinates\nIn cylindrical coordinates (\\rho, \\theta, z), we can write a vector field as\n{\\boldsymbol{f}}= f_\\rho {\\boldsymbol{e}}_\\rho + f_\\theta {\\boldsymbol{e}}_\\theta + f_z {\\boldsymbol{e}}_z,\nwhere f_\\rho is the radial component, f_\\theta the angular component and f_z is the axial component.\nThen, the differential operators can be written as\n\\nabla \\psi = \\frac{\\partial \\psi}{\\partial \\rho} {\\boldsymbol{e}}_\\rho + \\frac{1}{\\rho} \\frac{\\partial \\psi}{\\partial \\theta} {\\boldsymbol{e}}_\\theta + \\frac{\\partial \\psi}{\\partial z} {\\boldsymbol{e}}_z,\n\\nabla \\cdot {\\boldsymbol{f}}= \\frac{1}{\\rho} \\frac{\\partial (\\rho f_\\rho)}{\\partial \\rho} + \\frac{1}{\\rho} \\frac{\\partial f_\\theta}{\\partial \\theta} + \\frac{\\partial f_z}{\\partial z},\n\\begin{aligned}\n\\nabla \\times {\\boldsymbol{f}}&= \\left(\\frac{1}{\\rho} \\frac{\\partial f_z}{\\partial \\theta} - \\frac{\\partial f_\\theta}{\\partial z} \\right) {\\boldsymbol{e}}_\\rho \\\\\n& \\quad + \\left( \\frac{\\partial f_\\rho}{\\partial z} - \\frac{\\partial f_z}{\\partial \\rho} \\right) {\\boldsymbol{e}}_\\theta \\\\\n& \\quad + \\frac{1}{\\rho} \\left( \\frac{\\partial (\\rho f_\\theta)}{\\partial \\rho}  - \\frac{\\partial f_\\rho}{\\partial \\theta} \\right) {\\boldsymbol{e}}_z,\n\\end{aligned}\n\\nabla^2 \\psi = \\frac{1}{\\rho} \\frac{\\partial}{\\partial \\rho} \\left(\\rho \\frac{\\partial \\psi}{\\partial \\rho} \\right)  + \\frac{1}{\\rho^2} \\frac{\\partial^2 \\psi}{\\partial \\theta^2} + \\frac{\\partial^2 \\psi}{\\partial z^2}.\n\n\n1.1.4.2 Spherical coordinates\nIn spherical coordinates (\\rho, \\theta, \\varphi), we can write a vector field as\n{\\boldsymbol{f}}= f_\\rho {\\boldsymbol{e}}_\\rho + f_\\theta {\\boldsymbol{e}}_\\theta + f_\\varphi {\\boldsymbol{e}}_\\varphi,\nwhere f_\\rho is the radial component, f_\\theta the polar angular component and f_\\varphi is the azimuthal angular component.\nThen, the differential operators can be written as\n\\nabla \\psi = \\frac{\\partial \\psi}{\\partial \\rho} {\\boldsymbol{e}}_\\rho + \\frac{1}{\\rho} \\frac{\\partial \\psi}{\\partial \\theta} {\\boldsymbol{e}}_\\theta + \\frac{1}{\\rho \\sin \\theta} \\frac{\\partial \\psi}{\\partial \\varphi} {\\boldsymbol{e}}_\\varphi,\n\\nabla \\cdot {\\boldsymbol{f}}= \\frac{1}{\\rho^2} \\frac{\\partial (\\rho^2 f_\\rho)}{\\partial \\rho} + \\frac{1}{r \\sin \\theta} \\frac{\\partial}{\\partial \\theta} \\left( f_\\theta \\sin \\theta \\right) + \\frac{1}{\\rho \\sin \\theta} \\frac{\\partial f_\\varphi}{\\partial \\varphi},\n\\begin{aligned}\n\\nabla \\times {\\boldsymbol{f}}&= \\frac{1}{\\rho \\sin \\theta} \\left( \\frac{\\partial }{\\partial \\theta} (f_\\varphi \\sin \\theta) - \\frac{\\partial f_\\theta}{\\partial \\varphi} \\right) {\\boldsymbol{e}}_\\rho \\\\\n& \\quad + \\frac{1}{\\rho} \\left( \\frac{1}{\\sin \\theta} \\frac{\\partial f_\\rho}{\\partial \\varphi} -  \\frac{\\partial}{\\partial \\rho} (\\rho f_\\varphi) \\right) {\\boldsymbol{e}}_\\theta \\\\\n& \\quad + \\frac{1}{\\rho} \\left( \\frac{\\partial}{\\partial \\rho} (\\rho f_\\theta) - \\frac{\\partial f_\\rho}{\\partial \\theta} \\right) {\\boldsymbol{e}}_\\varphi,\n\\end{aligned}\n\\nabla^2 \\psi = \\frac{1}{\\rho^2} \\frac{\\partial}{\\partial \\rho} \\left( \\rho^2 \\frac{\\partial \\psi}{\\partial \\rho} \\right) + \\frac{1}{\\rho^2 \\sin \\theta} \\frac{\\partial}{\\partial \\theta} \\left( \\sin \\theta \\frac{\\partial \\psi}{\\partial \\theta} \\right) + \\frac{1}{\\rho^2 \\sin^2 \\theta} \\frac{\\partial^2 \\psi}{\\partial \\varphi^2}.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to electromagnetism</span>"
    ]
  },
  {
    "objectID": "1-introduction.html#footnotes",
    "href": "1-introduction.html#footnotes",
    "title": "1  Introduction to electromagnetism",
    "section": "",
    "text": "I am skipping the proofs here, but most of them are exercises in problem sheet 1.↩︎\nA Möbius strip is not orientable, because if we define a normal and then we continuously move that normal along the strip, once we get back to where we started it points in the other direction. Therefore, we cannot define a continuous unit normal.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to electromagnetism</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html",
    "href": "2-electrostatics.html",
    "title": "2  Electrostatics",
    "section": "",
    "text": "2.1 Point charges and Coulomb’s law\nWe start our journey in the field of electromagnetism with electrostatics, which is the branch of the subject concerned with stationary electric charges. This is the simplest case, but it is present in our day-to-day lives in many ways: from laser printing to explaining static cling (see Figure 2.1).\nAs you may already know, subatomic particles like the proton and electrons, have a physical property called electric charge. Electric charge can be positive or negative and it is quantised, that means that it comes in integer multiples of the elementary charge e which, in SI units, is defined as e \\approx 1.602 \\times 10^{-19} \\; \\mathrm{C} (this unit is called Coulomb1). Protons have a charge of +e while electrons have a charge of -e, and all charges in matter arise from them. Charges2 with the same sign attract each other, while charges of opposite sign repel each other.\nWe want to model the force between point charges. By point charges we mean that we will represent these electrically charges as points in \\mathbb{R}^3, with their position in space denoted by the vector {\\boldsymbol{r}}\\in \\mathbb{R}^3 and (the magnitude and sign of) their charge denoted by q_i.\nWe defined Coulomb’s law for a set of two charges. For three or more charges, we can use the Principle of Superposition.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#point-charges-and-coulombs-law",
    "href": "2-electrostatics.html#point-charges-and-coulombs-law",
    "title": "2  Electrostatics",
    "section": "",
    "text": "Definition 2.1 (Coulomb’s law) Given to point charges q_1 and q_2 positioned at {\\boldsymbol{r}}_1,{\\boldsymbol{r}}_2 \\in \\mathbb{R}^3, respectively, each charge experiences a force\n{\\boldsymbol{F}}_1 = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_1 q_2}{|{\\boldsymbol{r}}_1 - {\\boldsymbol{r}}_2|^2} {\\boldsymbol{e}}_{12} = - {\\boldsymbol{F}}_2, \\tag{2.1}\nwhere the constant \\epsilon_0 \\approx 8.854 \\times 10^{-12} \\; \\mathrm{C}^2 \\; \\mathrm{N}^{-1} \\; \\mathrm{m}^{-2} is called the permittivity of free space, and {\\boldsymbol{e}}_{12} is the unit vector pointing from q_2 to q_1, which can be written as {\\boldsymbol{e}}_{12} = \\frac{{\\boldsymbol{r}}_1 - {\\boldsymbol{r}}_2}{|{\\boldsymbol{r}}_1 - {\\boldsymbol{r}}_2|}.\n\n\nRemark 2.1. \n\nThe law only holds if {\\boldsymbol{r}}_1 \\neq {\\boldsymbol{r}}_2 (otherwise the force would be infinitely large), therefore charges cannot sit on top of each other.\nThe force that the first charge exerts on the second has the same magnitude and opposite direction to the force that the second charge exerts on the first one, thus satisfying Newton’s third law.\nYou may have noticed that Coulomb’s law is very similar to Newton’s law of universal gravitation (it is inversely proportional to the square of the distance between charges). However, charges (as opposed to masses) can take both positive and negative values. Therefore, the electrostatic force between charges can be attractive or repulsive.\nNeutral particles (that is particles without an electric charge) do not experience electromagnetic force.\n\n\n\n\nDefinition 2.2 (Principle of superposition) Given N point charges q_i at positions {\\boldsymbol{r}}_i, with i \\in {1, \\dots, N}, an additional charge q at position {\\boldsymbol{r}} experiences a force\n {\\boldsymbol{F}}= \\sum_{i=1}^N \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_i}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|^2}\\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}_i}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|}. \\tag{2.2}\nTherefore, the total force on charge q is the sum of the forces exerted by all other charges (i.e. we can superpose the forces from each charge).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#electric-fields-and-electrostatic-potentials",
    "href": "2-electrostatics.html#electric-fields-and-electrostatic-potentials",
    "title": "2  Electrostatics",
    "section": "2.2 Electric fields and electrostatic potentials",
    "text": "2.2 Electric fields and electrostatic potentials\nLet’s now introduce the concept of electric field. Even though it looks like a simple rewriting of the equation, it provides a very valuable new point of view. As you may recall, the electric field was one of the variables appearing in the Maxwell’s equations.\n\nDefinition 2.3 (Electric field) Given a set of charges q_i at positions {\\boldsymbol{r}}_i, with i \\in {1, \\dots, N}, we define the electric field at a given point {\\boldsymbol{r}}\\in \\mathbb{R}^3 as\n{\\boldsymbol{E}}({\\boldsymbol{r}}) = \\sum_{i=1}^N \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_i}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|^2}\\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}_i}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|}. \\tag{2.3}\n\nThe electric field can be interpreted as the force that a unit test charge (that is, a virtual charge with q=1) would experience at the point in space {\\boldsymbol{r}}. It is called a test charge as its charge is not part of the set of charges we are considering (we use it only to “test” how the set of charges behaves). The electric field is a vector field defined in {\\boldsymbol{r}}\\in \\mathbb{R}^3 \\setminus \\{{\\boldsymbol{r}}_1, \\dots, {\\boldsymbol{r}}_N\\}.\n\nRemark 2.2. Note that {\\boldsymbol{F}}= q {\\boldsymbol{E}} yields Equation 2.2. This expression also allows us to deduce that the the electric field is measured (in SI units) in \\mathrm{N} \\; \\mathrm{C}^{-1} (or, equivalently \\mathrm{V} \\; \\mathrm{m}^{-1}).\n\nNow is the turn to introduce another useful concept: the electrostatic potential. You are probably familiar with the concept of potential, for example the gravitational potential. In a similar manner, we can define the electrostatic potential.\n\nDefinition 2.4 (Electrostatic potential) The electrostatic potential for a given electric field {\\boldsymbol{E}} is defined as the function \\phi: \\mathbb{R}^3 \\to \\mathbb{R} such that {\\boldsymbol{E}}= -\\nabla \\phi.\n\n\nTheorem 2.1 For a single point charge q at {\\boldsymbol{r}}_0 the electrostatic potential is given by\n \\phi({\\boldsymbol{r}}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_0|}. \\tag{2.4}\n\nThe proof immediately follows from applying Definition 2.4 to Equation 2.4. This result can be easily extended to multiple point charge using the principle of superposition. We will further discuss the electrostatic potential in Section 2.5.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#gauss-law-for-point-charges",
    "href": "2-electrostatics.html#gauss-law-for-point-charges",
    "title": "2  Electrostatics",
    "section": "2.3 Gauss’ law for point charges",
    "text": "2.3 Gauss’ law for point charges\nAs we said earlier, the definition of the electric field is a lot more than a simple relabelling, and in this section we will see why. Before that, we need to recall a definition from vector calculus.\n\nDefinition 2.5 (Flux) Given a surface \\Sigma \\subset \\mathbb{R}^3 with outward3 unit normal vector {\\boldsymbol{n}}, we define the flux of the electric field {\\boldsymbol{E}} through \\Sigma as the integral \\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}S.\n\nWith this definition, we can now state one of the central results of electrostatics\n\nTheorem 2.2 (Gauss’ law) For any closed surface \\Sigma = \\partial \\Omega (that is the surface bounding a region \\Omega \\in \\mathbb{R}^3), we have\n\\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{1}{\\epsilon_0} \\sum_{i=1}^N q_i, \\tag{2.5}\nwhere q_i is the (finite4) set of charges contained in \\Omega. For convenience, we define the total charge in \\Omega as Q = \\sum_{i=1}^N q_i.\n\n\nProof. Let’s start by considering a single point charge q at a position {\\boldsymbol{r}}_0. Without loss of generality, we can assume {\\boldsymbol{r}}_0 = {\\boldsymbol{0}} as we can always perform a change of coordinates. From Equation 2.3 we find that the charge produces an electric field\n {\\boldsymbol{E}}({\\boldsymbol{r}}) = \\frac{q}{4 \\pi \\epsilon_0} \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3}. Let’s also consider a ball of arbitrary radius R &gt; 0 centred at the origin B_R, the flux of the electric field through its surface is5\n \\int_{\\partial B_R} {\\boldsymbol{E}}\\cdot \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|} \\; {\\mathrm{d}}A = \\frac{q}{4 \\pi \\epsilon_0} \\int_{\\partial B_R} \\frac{1}{|{\\boldsymbol{r}}|^2} {\\mathrm{d}}A = \\frac{q}{4 \\pi R^2 \\epsilon_0} \\int_{\\partial B_R} {\\mathrm{d}}A = \\frac{q}{\\epsilon_0}. \\tag{2.6}\nThis resembles the Gauss’ law, but for a single charge and a very specific surface, so we now need to generalise the result. From the divergence theorem (Theorem 1.2), for an arbitrary domain \\Omega bounded by \\Sigma = \\partial \\Omega we have\n \\int_\\Omega \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V = \\int_{\\Sigma} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A.\nLet’s now compute the divergence of the electric field\n \\nabla \\cdot {\\boldsymbol{E}}= \\frac{q}{4 \\pi \\epsilon_0} \\nabla \\cdot \\left( \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} \\right) = 0, \\tag{2.7}\nand note we have used Equation 1.15. However, the electric field is singular at {\\boldsymbol{r}}= {\\boldsymbol{0}}, so we can only state that the divergence of the electric field is zero for {\\boldsymbol{r}}\\in \\mathbb{R}^3 \\setminus \\{{\\boldsymbol{0}}\\}.\nNow let’s consider assume our arbitrary domain \\Omega \\in \\mathbb{R}^3 contains the ball B_R. The integral of Equation 2.7 over \\Omega can be split into\n\\int_\\Omega \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V = \\int_{\\Omega \\setminus B_R} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V + \\int_{B_R} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V.\nThe first term is zero, as the integration domain does not contain the origin, while we can apply the divergence theorem to the second term, obtaining\n\\int_\\Omega \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V = \\int_{B_R} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V = \\int_{\\partial B_R} {\\boldsymbol{E}}\\cdot {\\boldsymbol{r}}\\; {\\mathrm{d}}A = \\frac{q}{\\epsilon_0}, \\tag{2.8}\nwhere in the final step we have used Equation 2.6. Therefore\n \\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{q}{\\epsilon_0}. \\tag{2.9}\nNow consider a distribution of N charges q_i located at {\\boldsymbol{r}}_i, respectively. By the principle of superposition Equation 2.3 we can write the electric field as the sum of the contributions for each charge. Then, similar to Equation 2.7, we can show that the divergence of the electric field will be zero in \\mathbb{R}^3 \\setminus \\bigcup_i {\\boldsymbol{r}}_i. Defining as B_i as the ball of arbitrary radius R centred at {\\boldsymbol{r}}_i (i.e. around charge q_i), we can write\n\\int_\\Omega \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V = \\int_{\\Omega \\setminus \\bigcup_{i=1}^N B_i} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V + \\sum_{i=1}^N \\int_{B_i} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V.\nThe first term is still zero as it doesn’t include any of the charges. For each ball B_i we only need to consider the contribution of the charge q_i as it is the only singularity of the electric field in that ball, so Equation 2.8 generalises into\n \\int_{B_i} \\nabla \\cdot {\\boldsymbol{E}}\\; {\\mathrm{d}}V  = \\int_{\\partial B_i} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{q_i}{\\epsilon_0}.\nTherefore, we conclude\n\\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\sum_{i=1}^N \\frac{q_i}{\\epsilon_0}.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#charge-density",
    "href": "2-electrostatics.html#charge-density",
    "title": "2  Electrostatics",
    "section": "2.4 Charge density",
    "text": "2.4 Charge density\nSo far we have worked with a set of point charges. However, this is quite often impractical. For example, a macroscopic object has an enormous number of electrons and protons so it is not reasonable to consider them one by one. A more subtle point is that, from a quantum mechanics point of view, the position of electron cannot be determined and instead they should be treated as a “blur” of charge. In either case, the concept of charge density comes handy.\n\nDefinition 2.6 (Charge density) We define the charge density as a function \\rho({\\boldsymbol{r}}): \\Omega \\subset \\mathbb{R}^3 \\to \\mathbb{R} that gives the charge per unit volume at a certain point in space.\nWe can then define the total charge in any arbitrary region \\Omega \\subset \\mathbb{R}^3 as\nQ = \\int_\\Omega \\rho \\; {\\mathrm{d}}V.\n\nUnless stated otherwise, we will assume that any charge density function \\rho we encounter in this module is, at least, continuous. We will also assume that \\rho has support \\Omega and that it is bounded. We can now redefine the electric field Equation 2.3 in terms of the charge density.\n\nDefinition 2.7 (Electric field) Given a charge density \\rho : \\Omega \\subset \\mathbb{R}^3 \\to \\mathbb{R}, we define the electric field at a given point {\\boldsymbol{r}}\\in \\mathbb{R}^3 as\n {\\boldsymbol{E}}({\\boldsymbol{r}}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{\\rho({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^2} \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}'}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\; {\\mathrm{d}}V', \\tag{2.10}\nwhere {\\mathrm{d}}V' represents the infinitesimal volume element associated to the coordinate {\\boldsymbol{r}}'.\n\nSimilarly, we can redefine the electrostatic potential for a charge density.\n\nTheorem 2.3 (Electrostatic potential) The electrostatic potential for a charge density \\rho({\\boldsymbol{r}}) is given by\n \\phi({\\boldsymbol{r}}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{\\rho({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V'. \\tag{2.11}\n\n\nRemark 2.3. It’s worth pointing out that, as \\rho has compact support, \\phi \\sim r^{-1} as |{\\boldsymbol{r}}| \\to \\infty and thus it vanishes.\n\nThe proof also follows directly from Definition 2.7. Note that, when taking the gradient of \\phi, it acts on {\\boldsymbol{r}} only ({\\boldsymbol{r}}' is the integration variable and thus it’s “invisible” from outside the integral).\nIt should come as no surprise that we can also rewrite Gauss’ law for a charge density.\n\nTheorem 2.4 (Gauss’ law) For any closed surface \\Sigma = \\partial \\Omega (that is the surface bounding a region \\Omega \\in \\mathbb{R}^3), we have\n\\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{1}{\\epsilon_0} \\int_\\Omega \\rho \\; {\\mathrm{d}}V. \\tag{2.12}\n\nUsing the divergence theorem (Theorem 1.2) on Equation 2.12 and rearranging we can write\n \\int_\\Omega \\left(\\nabla \\cdot {\\boldsymbol{E}}- \\frac{\\rho}{\\epsilon_0}\\right) {\\mathrm{d}}V = 0,\nand given that this holds for any arbitrary domain \\Omega \\subset \\mathbb{R}^3 we obtain the Gauss’ law in differential form.\n\nTheorem 2.5 (Gauss’ law – differential form) The electric field generated by a given charge density \\rho : \\Omega \\subseteq \\mathbb{R}^3 \\to \\mathbb{R} satisfies\n \\nabla \\cdot {\\boldsymbol{E}}= \\frac{\\rho}{\\epsilon_0}. \\tag{2.13}\nThis is the first of the Maxwell’s equations (Equation 1.1).\n\nWe have now two sets of definitions for electric field and Gauss’ law: one for point charges and one for charge densities. But how do they relate to each other? The key is the Dirac delta function.\n\nDefinition 2.8 (Dirac delta function – 1D) The Dirac delta function \\delta(x) in 1D is defined to satisfy the following properties:\n \\delta (x) = 0 \\quad \\text{ if } x \\neq 0,\nand\n\\int_I \\delta(x) {\\mathrm{d}}x = \\begin{cases} 1 & \\text{if } 0 \\in I,\\\\ 0 & \\text{otherwise}.\\end{cases} \nIn words, it is a function that is equal to zero everywhere except at the origin (where it is infinitely large), and its integral over any interval I containing the origin6 is equal to one.\n\n\nRemark 2.4. One way of thinking about the Dirac delta is as the limit of a Gaussian probability distribution centred at the origin when the variance tends to zero (i.e. when you “squeeze” it).\n\n\nProposition 2.1 The Dirac delta function satisfies:\n\n\\int_{-\\infty}^{\\infty}  f(x') \\delta(x' - x) \\; {\\mathrm{d}}x' = f(x),\n\\delta ( a x ) = \\frac{1}{|a|} \\delta(x),\n\\int_{-\\infty}^x \\delta(x') \\; {\\mathrm{d}}x' = H(x),\n\nwhere f(x) is any continuous function, a \\neq 0 is a constant and H(x) is the Heaviside step function.\n\nThe proofs for these properties follow immediately from Definition 2.8. We can generalise the definition of the Dirac delta function to higher dimensions\n\nDefinition 2.9 (Dirac delta function) The n-dimensional version Dirac delta function \\delta: \\mathbb{R}^n \\to \\mathbb{R} is defined as\n \\delta({\\boldsymbol{r}}) = \\Pi_{i = 1}^n \\delta(x_i) = \\delta(x_1) \\delta(x_2) \\dots \\delta(x_n),\nwhere {\\boldsymbol{r}}= (x_1, x_2, \\dots, x_n) in Cartesian coordinates.\n\n\nProposition 2.2 The Dirac delta function in n-dimensions satisfies the properties\n\n \\delta ({\\boldsymbol{r}}) = 0 \\quad \\text{ if } {\\boldsymbol{r}}\\neq {\\boldsymbol{0}},\n\\int_\\Omega \\delta({\\boldsymbol{r}}) {\\mathrm{d}}V = \\begin{cases} 1 & \\text{if } {\\boldsymbol{0}}\\in \\Omega,\\\\ 0 & \\text{otherwise},\\end{cases} \n\\int_\\Omega  f({\\boldsymbol{r}}') \\delta({\\boldsymbol{r}}' - {\\boldsymbol{r}}) {\\mathrm{d}}V = f({\\boldsymbol{r}}),\n\n\nLet’s now get back to the connection between point charges and charge densities. We can think of a point charge q at {\\boldsymbol{r}}_0 as a charge density\n\\rho({\\boldsymbol{r}}) = q \\; \\delta({\\boldsymbol{r}}- {\\boldsymbol{r}}_0). \\tag{2.14}\nSubstituting this definition in Equation 2.10 we obtain\n{\\boldsymbol{E}}({\\boldsymbol{r}}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{q \\delta({\\boldsymbol{r}}' -{\\boldsymbol{r}}_0)}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^2} \\frac{{\\boldsymbol{r}}-{\\boldsymbol{r}}'}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V' = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_0|^2} \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}_0}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_0|},\nwhich corresponds to Equation 2.3 for a single point charge. We can proceed similarly for the electrostatic potential.\nFor Gauss’ law, we can substitute Equation 2.14 into Equation 2.12 and obtain\n\\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{1}{\\epsilon_0} \\int_\\Omega q \\delta({\\boldsymbol{r}}- {\\boldsymbol{r}}_0) \\; {\\mathrm{d}}V = \\frac{q}{\\epsilon_0},\nas, by assumption, {\\boldsymbol{r}}_0 \\in \\Omega. Therefore, we have recovered Equation 2.5 for a single point charge.\n\nProposition 2.3 The three-dimensional Dirac delta function can be written as\n\\delta ({\\boldsymbol{r}}- {\\boldsymbol{r}}') = - \\frac{1}{4\\pi} \\nabla^2 \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right).\n\n\nProof. We already know from Equation 1.14 and Equation 1.15 that in \\mathbb{R}^3 \\setminus \\{{\\boldsymbol{r}}'\\} we have\n\\nabla^2 \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) = 0,\nso we only need to consider what happens at {\\boldsymbol{r}}'. Let’s consider the integral of the term above in B_R, which is the ball of radius R centred at {\\boldsymbol{r}}'. We have\n\\begin{aligned}\n\\int_{B_R} \\nabla^2 \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right)\\;  {\\mathrm{d}}V &= \\int_{\\partial B_R} \\nabla \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A \\\\\n&= \\int_{\\partial B_R} \\left( - \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}'}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} \\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A \\\\\n&= - \\frac{4 \\pi \\epsilon_0}{q} \\int_{\\partial B_R} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A \\\\\n&= -4 \\pi,\n\\end{aligned}\nwhere in the first line we have used the divergence theorem, in the second line we have used Equation 1.14, in the third line we have used Definition 2.3 and in the last line we have used Theorem 2.2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#sec-electrostatic-potential",
    "href": "2-electrostatics.html#sec-electrostatic-potential",
    "title": "2  Electrostatics",
    "section": "2.5 More on the electrostatic potential",
    "text": "2.5 More on the electrostatic potential\nLet’s turn our attention back to the electrostatic potential. In Definition 2.4 we defined it in terms of the electric field, but it’s usually more convenient to compute it directly from the charge density.\n\nProposition 2.4 The electrostatic potential satisfies\n \\nabla ^ 2 \\phi = - \\frac{\\rho}{\\epsilon_0}.\nThis equation is known as Poisson’s equation.\n\n\nProof. Substituting Definition 2.4 into Equation 2.13 we get\n\\nabla \\cdot \\left( - \\nabla \\phi \\right) = \\frac{\\rho}{\\epsilon_0},\nand thus\n\\nabla^2 \\phi = - \\frac{\\rho}{\\epsilon_0}.\n\nThere is another interesting implication stemming from the definition of electrostatic potential. If {\\boldsymbol{E}}= - \\nabla \\phi we have\n\\nabla \\times {\\boldsymbol{E}}= \\nabla \\times \\left( \\nabla \\phi \\right) = 0, \\tag{2.15}\nas the curl of a gradient is always zero Equation 1.7. Note that this is the second of the Maxwell’s equations (Equation 1.3) for the steady-state case (i.e. \\frac{\\partial {\\boldsymbol{B}}}{\\partial t} = 0).\nYou may recall, for example when studying gravity, that when a force can be defined as the gradient of a scalar function (i.e. a potential), then the force is called conservative. Given that the electrostatic force is given by {\\boldsymbol{F}}= q {\\boldsymbol{E}}, and {\\boldsymbol{E}}= - \\nabla \\phi, it must be conservative. Then, we can compute the work7 done against the electrostatic force for a charge q moving along a path C starting at {\\boldsymbol{r}}_\\mathrm{i} and ending at {\\boldsymbol{r}}_\\mathrm{f} is\n W = - \\int_C {\\boldsymbol{F}}\\cdot {\\mathrm{d}}{\\boldsymbol{s}}= - q \\int_C {\\boldsymbol{E}}\\cdot {\\mathrm{d}}{\\boldsymbol{s}}= q \\int_C \\nabla \\phi \\cdot {\\mathrm{d}}{\\boldsymbol{s}}= q \\left[ \\phi ({\\boldsymbol{r}}_\\mathrm{f}) - \\phi ({\\boldsymbol{r}}_\\mathrm{i})\\right]. \\tag{2.16}\nTherefore, we deduce that the work done does not depend on the path C, only on its start and end points, and we can conclude that the electrostatic force is conservative.\n\nRemark 2.5. Note that the electrostatic potential is defined up to a constant. This means that, if \\phi is a potential for a given electric field {\\boldsymbol{E}}, then \\hat{\\phi} = \\phi + c (where c is a constant) is also a potential for {\\boldsymbol{E}}. Therefore, the quantity that is well-defined (and we can measure) physically is the potential difference between two points, which we call voltage.\nWhen working with electrostatic potentials, it is up to us to define a reference for the potential, that is the value of the potential at a given point, which allows us to fix the constant. For example, in Definition 2.4 we assumed that the potential is zero as |{\\boldsymbol{r}}| \\to \\infty.\n\n\nDefinition 2.10 (Field lines) Given a vector field, a field line is a line that at each point is tangent to the vector field.8\n\nField lines are a very useful way to visualise electric (and magnetic) fields.\n\nDefinition 2.11 (Equipotential surfaces) Surfaces of constant \\phi are called equipotentials.9\n\n\nProposition 2.5 The electric field (i.e. the field lines) are always normal to an equipotential surface.\n\n\nProof. Define {\\boldsymbol{t}} to be a tangent vector to the equipotential at a given point {\\boldsymbol{r}}. From the definition of the equipotential, we know that the derivative of \\phi in the tangent direction {\\boldsymbol{t}} must be zero (as the potential is constant along an equipotential surface). Thus (\\nabla \\phi) \\cdot {\\boldsymbol{t}}= 0 and from the definition of the electrostatic potential we conclude that {\\boldsymbol{E}}\\cdot {\\boldsymbol{t}}= 0 so the electric field is perpendicular to the equipotential surface (and so are the field lines).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#electrostatic-energy",
    "href": "2-electrostatics.html#electrostatic-energy",
    "title": "2  Electrostatics",
    "section": "2.6 Electrostatic energy",
    "text": "2.6 Electrostatic energy\nTo finish this chapter on electrostatics, let’s talk about the electrostatic energy. Note that the electrostatic potential at a given point can be interpreted as the potential energy required to bring a unit charge to that point from some reference point (usually infinity).\nHowever, we want to extend this concept from a single charge to any electrostatic configuration. As usual, let’s start considering point charges to build intuition about the general form. We start with a charge q_1 at {\\boldsymbol{r}}_1. Its potential is\n \\phi_1 = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_1|}. \nNow let’s consider a charge q_2, which we move from infinity to a point {\\boldsymbol{r}}_2. From Equation 2.16, the work done against the electric field is\n W_2 = q_2 \\; \\phi_1({\\boldsymbol{r}}_2) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_2 q_1}{|{\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1|}.\nLet’s now do the same for a charge q_3 that needs to be placed at {\\boldsymbol{r}}_3, in the presence of q_1 and q_2:\nW_3 = q_3 (\\phi_1 ({\\boldsymbol{r}}_3) + \\phi_2 ({\\boldsymbol{r}}_3)) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_3 q_1}{|{\\boldsymbol{r}}_3 - {\\boldsymbol{r}}_1|} + \\frac{q_3 q_2}{|{\\boldsymbol{r}}_3 - {\\boldsymbol{r}}_2|} \\right),\nwhich we deduce from the principle of superposition (\\phi_2 is the potential of the charge q_2). The total work done so far is W = W_2 + W_3. By induction, we can infer that for N charges q_1, \\dots, q_N at {\\boldsymbol{r}}_1,\\dots,{\\boldsymbol{r}}_N, respectively, the total work to assemble them is\n W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i=1}^N \\sum_{j &lt; i} \\frac{q_i q_j}{|{\\boldsymbol{r}}_i - {\\boldsymbol{r}}_j|}.\nBy symmetry on i and j, we can write\nW = \\frac{1}{2} \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i=1}^N \\sum_{j \\neq i} \\frac{q_i q_j}{|{\\boldsymbol{r}}_i - {\\boldsymbol{r}}_j|}.\nRewriting it in terms of electrostatic potentials we get\nW = \\frac{1}{2} \\sum_{i=1}^N q_i \\phi^{(i)}, \\tag{2.17}\nwhere\n\\phi^{(i)} = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{j \\neq i} \\frac{q_j}{|{\\boldsymbol{r}}_i - {\\boldsymbol{r}}_j|}\nis defined as the potential generated by all charges except for q_i evaluated at {\\boldsymbol{r}}_i.\nNow let’s take the continuum limit, to consider the potential created by a charge density instead (Equation 2.11). Then Equation 2.17 becomes\nW = \\frac{1}{2} \\int_\\Omega \\rho \\phi \\; {\\mathrm{d}}V. \\tag{2.18}\nUsing Gauss’ law (Equation 2.13) and Equation 1.13, we obtain\n\\phi \\frac{\\rho}{\\epsilon_0} = \\phi \\nabla \\cdot {\\boldsymbol{E}}= \\nabla \\cdot (\\phi {\\boldsymbol{E}}) - \\nabla \\phi \\cdot {\\boldsymbol{E}}= \\nabla \\cdot (\\phi {\\boldsymbol{E}}) + {\\boldsymbol{E}}\\cdot {\\boldsymbol{E}}= \\nabla \\cdot (\\phi {\\boldsymbol{E}}) + | {\\boldsymbol{E}}|^2.\nSubstituting into Equation 2.18 and using the divergence theorem we get\nW = \\frac{\\epsilon_0}{2} \\left( \\int_\\Sigma \\phi {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A + \\int_\\Omega  \n| {\\boldsymbol{E}}|^2 \\; {\\mathrm{d}}V \\right). \\tag{2.19}\nAs we want to know the energy of the whole charge configuration, we take \\Omega to be a ball centred at the origin with radius r. Taking the limit r \\to \\infty and recalling that in that limit \\phi \\sim \\frac{1}{r}, the surface integral term vanishes, and Equation 2.19 reduces to\n W = \\frac{\\epsilon_0}{2} \\int_{{\\mathbb{R}}^3} | {\\boldsymbol{E}}|^2 \\; {\\mathrm{d}}V.\nIf the integral exists, we say that the charge configuration has finite energy W.\n\nDefinition 2.12 (Energy density) The energy density of a charge configuration is defined by\n\\mathcal{E} = \\frac{\\epsilon_0}{2} | {\\boldsymbol{E}}|^2,\nand the total energy of the configuration can be written as\nW = \\int_{{\\mathbb{R}}^3} \\mathcal{E} {\\mathrm{d}}V.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "2-electrostatics.html#footnotes",
    "href": "2-electrostatics.html#footnotes",
    "title": "2  Electrostatics",
    "section": "",
    "text": "The unit is named after the french physicist Charles-Augustin de Coulomb, who formulated what today is known as Coulomb’s law (coming next).↩︎\nAs a shorthand, we will refer to particles with an electrical charge as “charges”.↩︎\nFor a closed surface it is quite intuitive what outward means: pointing into the unbounded domain. For an open surface, the definition is arbitrary and we get to choose it. Note from the definition that this will simply change the sign of the flux.↩︎\nWe will consider what happens with infinite amounts of charge in the next section.↩︎\nRecall that the outwards normal unit vector to the sphere is \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|}.↩︎\nThis includes \\mathbb{R}.↩︎\nRecall that the work is defined as the energy transferred to or from an object via the application of force along a displacement. Note the importance of the displacement, as if there is no displacement, there is no work.↩︎\nIf you are taking MA3D1, you will notice that the streamlines are another example of field lines, in this case for a velocity field.↩︎\nIf considering a 2D problem, equipotential surfaces reduce to equipotential curves.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html",
    "href": "3-applications-electrostatics.html",
    "title": "3  Applications of electrostatics",
    "section": "",
    "text": "3.1 Equilibrium in an electrostatic field\nIn the previous chapter we introduced some machinery to understand electrostatics. The aim of this chapter is to put this machinery to work, so we can use it to understand some of the phenomena we see and experience in our day-to-day lives.\nLet’s first consider whether we can reach equilibrium in an electrostatic field, i.e. can we “trap” an electric charge at a given point just by placing other charges in the right places? To be precise, by “trap” we mean that the charge will lie in stable equilibrium at that point, meaning that if we slightly perturb it, it will go back to place (instead of flying away). We also do not consider the case where two charges lie on top of each other, which would actually give us equilibrium.\nYou can try as hard as you want to find a charge arrangement that would give us electrostatic equilibrium, but it is actually impossible. Let’s prove it by assuming otherwise and reaching a contradiction.\nAssume there is a charge configuration in which there is a point in empty space {\\boldsymbol{r}}_* that is stable for a charge q (wlog2 we assume q &gt; 0). Empty space means that the charge density is zero in a neighbourhood of {\\boldsymbol{r}}_*. As the point is stable, if we place the particle slightly off {\\boldsymbol{r}}_* it should be pushed back into position by the electric field. Therefore the particle should see an inwards force towards {\\boldsymbol{r}}_*. Therefore, for an arbitrary surface \\Sigma surrounding {\\boldsymbol{r}}_* and contained in the free space, we must have\n\\int_\\Sigma {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A &lt; 0.\nHowever, by Gauss’ law, the right-hand side must be proportional to the charge contained within \\Sigma, which we assumed was zero. This is a contradiction, and therefore electrostatic equilibrium is not possible.\nNote that, by using forces other than the electrostatic ones, one can construct equilibrium configurations. For example, in Example 3.1, if we constrain the new charge to lie in the horizontal axis (by imposing some sort of force), then we have a stable configuration as for a small displacement \\delta along the horizontal axis (in either direction) will result into a force in the horizontal direction with magnitude\nF = -\\frac{q}{\\pi \\epsilon_0} \\frac{\\delta}{(\\delta - 1)^2 (\\delta + 1)^2},\npushing the charge back to the origin.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#equilibrium-in-an-electrostatic-field",
    "href": "3-applications-electrostatics.html#equilibrium-in-an-electrostatic-field",
    "title": "3  Applications of electrostatics",
    "section": "",
    "text": "Example 3.1 Consider two point charges of charge q located at {\\boldsymbol{r}}_+=(1,0) and {\\boldsymbol{r}}_-=(-1,0), respectively.1 We assume these charges remain fixed in place somehow. By the principle of superposition, the electric field generated by these two charges is\n{\\boldsymbol{E}}({\\boldsymbol{r}}) = \\frac{q}{4 \\pi \\epsilon_0} \\left( \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}_+}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_+|^3} + \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}_-}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_-|^3} \\right).\nNow let’s introduce another charge q at the origin. The force this charge will experience is\n{\\boldsymbol{F}}= q {\\boldsymbol{E}}({\\boldsymbol{0}}) = {\\boldsymbol{0}},\nso the new charge will remain at the origin (no force acting on it). However, if we perturb the charge position, for example placing it at (0, \\delta), where \\delta &gt; 0 is a small quantity, the force becomes\n {\\boldsymbol{F}}= \\left( 0, \\frac{q^2}{2 \\pi \\epsilon_0} \\frac{\\delta}{(1 + \\delta^2)^\\frac{3}{2}}\\right), \nand thus the charge will be pushed upwards, away from the origin. Therefore, we found an equilibrium point, but not a stable one, not meeting our definition of “trapping”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#sec-conductors",
    "href": "3-applications-electrostatics.html#sec-conductors",
    "title": "3  Applications of electrostatics",
    "section": "3.2 Conductors",
    "text": "3.2 Conductors\nLet’s now talk about conductors.\n\nDefinition 3.1 (Electrical Conductor) An electrical conductor (or conductor, in short) is a material in which some charges are free to move (e.g. electrons in a metal). We will describe it mathematically as a given region in space that contains charges which can move freely.3\n\nThe are several consequences stemming from this definition, let’s unpack them one by one.\n\nProposition 3.1 In a static configuration, the electric field inside a conductor is equal to zero, i.e. {\\boldsymbol{E}}\\equiv {\\boldsymbol{0}}.\n\n\nProof. If the electric field inside the conductor was non-zero, any charges inside would move, but we do not allow it as we are considering a static configuration.\n\nFrom Proposition 3.1, we can deduce a series of corollaries.\n\nCorollary 3.1 The electrostatic potential \\phi inside a conductor must be constant (proof follows directly from the definition of \\phi).\n\n\nCorollary 3.2 The charge density inside the conductor must be identically zero, i.e. \\rho \\equiv 0 (proof follows directly from Gauss’ law). Therefore, any net charge of a conductor must sit at its surface.\n\n\nCorollary 3.3 The surface of the conductor is an equipotential (proof follows directly from \\phi = \\text{const}). Therefore, the electric field is perpendicular to the surface, which agrees with the fact that surface charges must be static.\n\nThe physical interpretation of these results is that, when a conductor is in the presence of an electric field, its free charges will rearrange in such a way (across the conductor surface) so they cancel out the electric field inside of the conductor.\nAll this naturally leads to the introduction of surface charges. Similar to the charge density \\rho, which is defined as charge per unit of volume, we can define the surface charge density \\sigma as the charge per unit of area. Then, we can naturally extend Definition 2.7 for a charge density \\sigma on a surface \\Sigma as\n\n{\\boldsymbol{E}}({\\boldsymbol{r}}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Sigma \\frac{\\sigma({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^2} \\frac{{\\boldsymbol{r}}- {\\boldsymbol{r}}'}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}A'.\n\\tag{3.1}\nHowever, now the electric field has some interesting properties.\n\nProposition 3.2 Consider the electric field Equation 3.1 generated by a surface charge density \\sigma on a surface \\Sigma. Then the electric field {\\boldsymbol{E}} must be continuous across \\Sigma in the tangential direction, but not in the normal direction.\nIn particular, the discontinuity can be written as\n{\\boldsymbol{E}}^+ \\cdot {\\boldsymbol{n}}- {\\boldsymbol{E}}^- \\cdot {\\boldsymbol{n}}= \\frac{\\sigma}{\\epsilon_0}. \\tag{3.2}\nHere the subscripts + and - represent different sides of the surface (arbitrarily labelled), and {\\boldsymbol{n}} is the normal unit vector to the surface pointing into the + side.\n\n\nProof. \n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3.1: Diagrams for the (a) Gaussian cylinder and the (b) loop used for the proof.\n\n\n\nLet’s start with the discontinuity in the normal direction. Consider a small cylinder \\Omega of height \\delta H and cross-sectional area \\delta A, as shown in Figure 3.1 (a). For simplicity we will assume that \\Sigma is flat in the intersection with the cylinder, the cylinder axis is perpendicular to the surface, and the surface intersects the cylinder at its midpoint; but the same argument holds for more generic situations.4 By Gauss’ law (#thm-Gauss-law-charge-density)\n \\int_{\\partial \\Omega} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{Q}{\\epsilon_0}, \\tag{3.3}\nwhere, remember, Q is the total charge inside the cylinder \\Omega. Let’s now decompose the surface of the cylinder into three surfaces: the “top lid” \\partial \\Omega^+, the “bottom lid” \\partial \\Omega^-, and the side \\partial \\Omega_\\mathrm{side}. We can split the integral into\n \\int_{\\partial \\Omega} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_{\\partial \\Omega^+} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}^+ \\; {\\mathrm{d}}A + \\int_{\\partial \\Omega^-} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}^- \\; {\\mathrm{d}}A + \\int_{\\partial \\Omega_\\mathrm{side}} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}_\\mathrm{side} \\; {\\mathrm{d}}A.\nNow we take the limit \\delta \\to 0 (i.e. we collapse the cylinder onto the surface). Then, the side component vanishes, and the top and bottom lid collapse into each other (and onto the surface). We can rewrite Equation 3.3 as\n\\int_{\\partial \\Omega^+} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}^+ \\; {\\mathrm{d}}A + \\int_{\\partial \\Omega^-} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}^- \\; {\\mathrm{d}}A = \\int_{\\partial \\Omega^+} \\left({\\boldsymbol{E}}^+ \\cdot {\\boldsymbol{n}}^+\\right) - \\left({\\boldsymbol{E}}^- \\cdot {\\boldsymbol{n}}^+\\right) \\; {\\mathrm{d}}A,\nwhere we have used that \\partial \\Omega^+ \\equiv \\partial \\Omega^- but {\\boldsymbol{n}}^+ = - {\\boldsymbol{n}}^-. The total charge Q is given by\n Q = \\int_{\\partial \\Omega^+} \\sigma \\; {\\mathrm{d}}A,\nso combining everything we obtain\n \\int_{\\partial \\Omega^+} \\left({\\boldsymbol{E}}^+ \\cdot {\\boldsymbol{n}}^+\\right) - \\left({\\boldsymbol{E}}^- \\cdot {\\boldsymbol{n}}^+\\right) \\; {\\mathrm{d}}A = \\int_{\\partial \\Omega^+} \\frac{\\sigma}{\\epsilon_0} {\\mathrm{d}}A.\nAs this equality must hold for any cylinder \\Omega, we conclude that\n{\\boldsymbol{E}}^+ \\cdot {\\boldsymbol{n}}- {\\boldsymbol{E}}^- \\cdot {\\boldsymbol{n}}= \\frac{\\sigma}{\\epsilon_0}.\nNow let’s show that {\\boldsymbol{E}} must be continuous in the tangential direction. Consider, instead, the rectangular loop C in Figure 3.1 (b), with height \\delta H and width \\delta L. The surface enclosed by the loop is defined as S. We compute the integral of E around the loop and apply Stokes’ theorem:\n\\int_C {\\boldsymbol{E}}\\cdot {\\boldsymbol{t}}\\; {\\mathrm{d}}s = \\int_S \\left( \\nabla \\times {\\boldsymbol{E}}\\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A.\nBut from Equation 2.15 we have that \\nabla \\times {\\boldsymbol{E}}= {\\boldsymbol{0}} so the integral needs to be equal to zero. By taking \\delta H \\to 0 and applying a similar argument as we did with the cylinder, we obtain\n{\\boldsymbol{E}}^+ \\cdot {\\boldsymbol{t}}- {\\boldsymbol{E}}^- \\cdot {\\boldsymbol{t}}= 0,\nand thus {\\boldsymbol{E}} is continuous in the tangential direction.\n\n\nRemark 3.1. Similarly to a surface charge density, we can define a line charge density \\lambda, where represents the charge per unit length along a line.\n\n\n3.2.1 Examples\n\nExample 3.2 (Line charge)  \n\n\n\n\n\n\nFigure 3.2: Diagram of the Gaussian surface used to compute the electric field of a line charge.\n\n\n\nConsider an infinitely long line charge distribution, with charge density \\lambda. We want to compute the electric field it produces. We will work in cylindrical polar coordinates and, wlog, we will assume the charge is distribution along the z axis (i.e. at r = 0).\nBy symmetry, we can conclude that the electric field should only depend on the radial coordinate and, moreover, the only non-zero component of the electric field is also along the radial coordinate. Therefore {\\boldsymbol{E}}= E(r) {\\boldsymbol{e}}_r.\nWe will again use a Gaussian surface for the argument, but this time we take an arbitrary cylinder of radius r and length L with its axis along the z axis, as shown in Figure 3.2. By Gauss’ law, we have\n\\int_{\\partial \\Omega} {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = E(r) 2 \\pi r L =  \\frac{\\lambda L}{\\epsilon_0},\nand thus\nE(r) = \\frac{\\lambda}{2 \\pi \\epsilon_0 r}.\nSo we conclude that the electric field is given by\n{\\boldsymbol{E}}= \\frac{\\lambda}{2 \\pi \\epsilon_0 r} {\\boldsymbol{e}}_r.\n\n\nExample 3.3 (Parallel plate capacitor)  \n\n\n\nSketch of a parallel plate capacitor.\n\n\nLet’s now consider two flat parallel surfaces. We will take the plates to be perpendicular to the x axis, and a distance d apart (wlog, we take them to be at x=0 and x=d). The plates have an area A, and we assume that d \\ll \\sqrt{A}, so we can ignore any effects arising from the edge of the plates.5\nEach plate has a total charge Q, but with opposite sign. Therefore, the surface charge distribution is defined as \\pm \\sigma = \\pm Q/A. We know6 that the electric field produced by an infinite plate with surface charge density \\sigma is uniform and perpendicular to the plate (pointing away from it if \\sigma &gt; 0) with magnitude\n | {\\boldsymbol{E}}| = \\frac{|\\sigma|}{2 \\epsilon_0}.\nThen, by superposition, the electric field produced by the two plates is\n {\\boldsymbol{E}}= \\begin{cases}\n\\frac{\\sigma}{\\epsilon_0} {\\boldsymbol{e}}_x, & \\text{between the plates},\\\\\n{\\boldsymbol{0}}, & \\text{otherwise}.\n\\end{cases}\nThis configuration is known as a capacitor, and it is typically used to store small amounts of energy (and many other things). We can then consider what’s the electrostatic potential in the capacitor. As the electric field is only in the x direction, we have\n \\frac{{\\mathrm{d}}\\phi}{{\\mathrm{d}}x} = - E \\implies \\phi = - \\frac{\\sigma}{\\epsilon_0} x + c,\nwhere c is an integration constant. We define the voltage or potential difference as the difference in potential between two points. In this case, the voltage between the two plates is\nV = \\phi(0) - \\phi(d) = \\frac{\\sigma d}{\\epsilon_0}.\nWe can then define the capacitance C of the capacitor as the charge in the capacitor divided by the potential difference:\nC = \\frac{Q}{V} = \\frac{A \\epsilon_0}{d}.\nThe capacitance dictates how much energy the capacitor can store, from Definition 2.12:\nW = \\frac{\\epsilon_0 A}{2} \\int_0^d \\left( \\frac{\\sigma}{\\epsilon_0} \\right)^2 {\\mathrm{d}}x = \\frac{Q^2}{2C}.\n\n\n\n3.2.2 Application: the Faraday cage\nLet’s use what we have learned about conductors to understand how a Faraday cage works. Consider a box made of a conducting material. Inside the box there are no charges. As shown in Figure 3.3, when we apply an electric field on the outside of the box, the charges on the box will redistribute to produce an electric field that will cancel out the field inside of the box. Therefore, anything or anyone inside the box is shielded from any external electric fields.\n\n\n\n\n\n\nFigure 3.3: Diagram of a Faraday cage in operation. MikeRun, CC BY-SA 4.0, via Wikimedia Commons\n\n\n\nAs the name suggests, though, Faraday cages are typically cages, not boxes, but the effect still holds. The mathematical analysis in that case is a bit more complicated, though. If you want to learn more, you can read this article by Jon Chapman, Dave Hewett and Nick Trefethen.\nFaraday cages play an important role in everyday life by shielding sensitive electronic devices from electromagnetic interference. A familiar example is the way cars act as Faraday cages, protecting occupants from lightning strikes during storms. Lightning is a sudden discharge between very strongly electrically charged areas (in this case, the clouds and the ground). Therefore, the lightning is just the consequence of a very strong electric field. This video by Top Gear is an entertaining (though a bit dated now) demonstration about a car behaving like a Faraday cage.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#boundary-value-problems",
    "href": "3-applications-electrostatics.html#boundary-value-problems",
    "title": "3  Applications of electrostatics",
    "section": "3.3 Boundary value problems",
    "text": "3.3 Boundary value problems\nSo far, we have considered problems where we are given a charge distribution \\rho everywhere in space, and we just computed the potential \\phi from Theorem 2.3. Instead, we now want to consider situations where the charge distribution is given in a region of space \\Omega only, along with some conditions on the potential at the boundary \\partial \\Omega. This type of problems are called, quite naturally, boundary value problems. More intuitively, it means that we will assume we have an infinite source/sink of charges outside \\Omega, and that they will do whatever they need to do to satisfy the imposed boundary condition.\nIn practice, this means solving Proposition 2.4, i.e.\n \\nabla ^ 2 \\phi = - \\frac{\\rho}{\\epsilon_0}, \\tag{3.4}\nalong some boundary conditions. There are two types of boundary conditions we will consider.\n\nDefinition 3.2 (Dirichlet boundary conditions) The value of \\phi is prescribed on \\partial \\Omega. Physically, this represents that the boundary is made of a conductive material.\n\n\nDefinition 3.3 (Neumann boundary conditions) The value of \\frac{\\partial \\phi}{\\partial n} = \\nabla \\phi \\cdot {\\boldsymbol{n}} is prescribed on \\partial \\Omega. This is equivalent to fixing {\\boldsymbol{E}}\\cdot {\\boldsymbol{n}}. Physically, this represents that the boundary is made of an insulating material.\n\nOne could also mix the boundary conditions, having Dirichlet boundary conditions on part of the boundary and Neumann conditions in the rest.\nIt’s natural to wonder whether Equation 3.4 with Dirichlet or Neumann (or mixed) boundary conditions has a unique solution.\n\nTheorem 3.1 The Poisson equation (Equation 3.4) with either Dirichlet or Neumann boundary conditions has a unique solution for \\phi. For Neumann boundary conditions, this solution is defined up to an arbitrary (or unphysical) additive constant.\n\n\nProof. Suppose that there are two solutions \\phi_1 and \\phi_2 to the problem. Now define the function f = \\phi_1 - \\phi_2. It should satisfy\n\\nabla^2 f = 0 \\tag{3.5}\nwith either zero Dirichlet or Neumann boundary conditions.\nConsider the identity\n \\nabla \\cdot \\left(f \\nabla f \\right) = |\\nabla f |^2 + f \\nabla^2 f,\nwhich is a corollary of Equation 1.13. Multiplying Equation 3.5 by f and integrating over \\Omega we get\n 0 = \\int_\\Omega f \\nabla^2 f {\\mathrm{d}}V = \\int_\\Omega \\left( \\nabla \\cdot \\left(f \\nabla f \\right) - |\\nabla f|^2 \\right) {\\mathrm{d}}V = \\int_{\\partial \\Omega} f \\nabla f \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A - \\int_\\Omega |\\nabla f|^2 {\\mathrm{d}}V,\nwhere in the last step we have used the divergence theorem on the first term in the integral. The surface integral that stems from it vanishes, as at the boundary either f = 0 or \\nabla f \\cdot {\\boldsymbol{n}}= 0 (by the boundary conditions). Then, we conclude\n\\int_\\Omega |\\nabla f|^2 {\\mathrm{d}}V = 0,\nand therefore we must have \\nabla f = {\\boldsymbol{0}} everywhere in \\Omega, which means f is constant. If we have Dirichlet boundary conditions (even if only at a subset of the boundary) we can conclude f = 0, if we have Neumann conditions everywhere, then we can’t determine the value of the constant. However, this constant has no physical meaning: we can arbitrarily define the origin of the potential, as the only thing that matters are the potential differences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#greens-functions",
    "href": "3-applications-electrostatics.html#greens-functions",
    "title": "3  Applications of electrostatics",
    "section": "3.4 Green’s functions",
    "text": "3.4 Green’s functions\nLet’s now discuss some methods to solve boundary value problems. We will start with Green’s functions.\n\nDefinition 3.4 (Green’s function) A Green’s function for Poisson’s equation is a function G({\\boldsymbol{r}}, {\\boldsymbol{r}}') satisfying\n\\nabla'^2 G({\\boldsymbol{r}}, {\\boldsymbol{r}}') = - 4 \\pi \\delta ({\\boldsymbol{r}}- {\\boldsymbol{r}}'), \\tag{3.6}\nwhere {\\boldsymbol{r}},{\\boldsymbol{r}}' \\in \\Omega. Note that the Laplacian \\nabla'^2 is defined in terms of the prime variable. This means it does not affect {\\boldsymbol{r}}, only {\\boldsymbol{r}}'.\n\nFrom Proposition 2.3 we know that a particular solution to Equation 3.6 is\nG({\\boldsymbol{r}}, {\\boldsymbol{r}}') = \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|}.\nNote, however, that this solution is not unique. The general solution to Equation 3.6 is any function\nG({\\boldsymbol{r}}, {\\boldsymbol{r}}') = \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} + F({\\boldsymbol{r}}, {\\boldsymbol{r}}'), \\tag{3.7}\nwhere F({\\boldsymbol{r}}, {\\boldsymbol{r}}') solves\n\\nabla'^2 F({\\boldsymbol{r}}, {\\boldsymbol{r}}') = 0.\n\nProposition 3.3 The solution of Equation 3.4 in a region \\Omega can be written as\n\\phi({\\boldsymbol{r}}) = \\frac{1}{4\\pi \\epsilon_0} \\int_\\Omega G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\rho ({\\boldsymbol{r}}') \\; {\\mathrm{d}}V' + \\frac{1}{4\\pi} \\int_{\\partial \\Omega} \\left( G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\frac{\\partial \\phi({\\boldsymbol{r}}')}{\\partial n'} - \\phi({\\boldsymbol{r}}') \\frac{\\partial G({\\boldsymbol{r}},{\\boldsymbol{r}}')}{\\partial n'} \\right) \\; {\\mathrm{d}}A'. \\tag{3.8}\n\n\nProof. By the properties of the divergence (similar to the argument in the proof of Theorem 3.1), we have that\n\\phi({\\boldsymbol{r}}') \\nabla'^2 G({\\boldsymbol{r}}, {\\boldsymbol{r}}') - G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\nabla'^2 \\phi({\\boldsymbol{r}}') = \\nabla' \\cdot\\left(\\phi({\\boldsymbol{r}}') \\nabla' G({\\boldsymbol{r}}, {\\boldsymbol{r}}') - G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\nabla' \\phi({\\boldsymbol{r}}')\\right).\nIntegrating over \\Omega and applying the divergence theorem on the right hand side we obtain\n \\int_\\Omega \\left( \\phi({\\boldsymbol{r}}') \\nabla'^2 G({\\boldsymbol{r}}, {\\boldsymbol{r}}') - G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\nabla'^2 \\phi({\\boldsymbol{r}}') \\right) {\\mathrm{d}}V' = \\int_{\\partial \\Omega} \\left( \\phi({\\boldsymbol{r}}') \\nabla' G({\\boldsymbol{r}}, {\\boldsymbol{r}}') - G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\nabla' \\phi({\\boldsymbol{r}}') \\right) \\cdot {\\boldsymbol{n}}' \\; {\\mathrm{d}}A'.\nBy Equation 3.6 and Equation 3.4 we can rewrite the left hand side as\n \\int_\\Omega \\left( \\phi({\\boldsymbol{r}}') \\nabla'^2 G({\\boldsymbol{r}}, {\\boldsymbol{r}}') - G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\nabla'^2 \\phi({\\boldsymbol{r}}') \\right) {\\mathrm{d}}V' = -4 \\pi \\phi({\\boldsymbol{r}}) + \\frac{1}{\\epsilon_0} \\int_\\Omega G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\rho({\\boldsymbol{r}}') \\; {\\mathrm{d}}V'. \nAfter some manipulation, we conclude\n\\phi({\\boldsymbol{r}}) = \\frac{1}{4\\pi \\epsilon_0} \\int_\\Omega G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\rho ({\\boldsymbol{r}}') {\\mathrm{d}}V' + \\frac{1}{4\\pi} \\int_{\\partial \\Omega} \\left( G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\frac{\\partial \\phi({\\boldsymbol{r}}')}{\\partial n'} - \\phi({\\boldsymbol{r}}') \\frac{\\partial G({\\boldsymbol{r}},{\\boldsymbol{r}}')}{\\partial n'} \\right) {\\mathrm{d}}A',\nwhere, for convenience, we have rewritten \\nabla'G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\cdot {\\boldsymbol{n}} as \\frac{\\partial G({\\boldsymbol{r}},{\\boldsymbol{r}}')}{\\partial n'}, and similarly for \\phi.\n\nThis result holds for any choice of the Green’s function which, remember, is not uniquely defined. Therefore we will try to make a suitable choice that simplifies Equation 3.8. For example, note that the second integral requires both \\phi and \\frac{\\partial \\phi}{\\partial n'}. If we have Dirichlet boundary conditions we will have the former but not the latter, and viceversa for Neumann boundary conditions. Therefore our choice of G will be aimed at eliminating the term for which we do not have information.\n\nDefinition 3.5 For Dirichlet boundary conditions, it is convenient to choose the Green’s function G_D({\\boldsymbol{r}}, {\\boldsymbol{r}}') to satisfy homogeneous Dirichlet boundary conditions, i.e.\nG_D({\\boldsymbol{r}}, {\\boldsymbol{r}}') = 0, \\quad \\text{for } {\\boldsymbol{r}}' \\in \\partial \\Omega, \\; {\\boldsymbol{r}}\\in \\Omega.\nThen, Equation 3.8 reduces to\n\\phi({\\boldsymbol{r}}) = \\frac{1}{4\\pi \\epsilon_0} \\int_\\Omega G_D({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\rho ({\\boldsymbol{r}}') \\; {\\mathrm{d}}V' - \\frac{1}{4\\pi} \\int_{\\partial \\Omega} \\phi({\\boldsymbol{r}}') \\frac{\\partial G_D({\\boldsymbol{r}},{\\boldsymbol{r}}')}{\\partial n'} \\; {\\mathrm{d}}A'.\n\n\nDefinition 3.6 For Neumann boundary conditions, it is convenient to choose the Green’s function G_N({\\boldsymbol{r}}, {\\boldsymbol{r}}') to satisfy\n\\frac{\\partial G_D}{\\partial n'} = - \\frac{4\\pi}{A}, \\quad \\text{for } {\\boldsymbol{r}}' \\in \\partial \\Omega, \\; {\\boldsymbol{r}}\\in \\Omega,\nwhere A = \\int_{\\partial \\Omega} {\\mathrm{d}}A' is the area of \\partial \\Omega. The reason for this choice is that the surface integral of \\frac{\\partial G_D}{\\partial n'} needs to be 4\\pi, otherwise Equation 3.6 would not be satisfied.\nThen, Equation 3.8 reduces to\n\\phi({\\boldsymbol{r}}) = \\frac{1}{4\\pi \\epsilon_0} \\int_\\Omega G_N({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\rho ({\\boldsymbol{r}}') \\; {\\mathrm{d}}V' + \\frac{1}{4\\pi} \\int_{\\partial \\Omega} G({\\boldsymbol{r}}, {\\boldsymbol{r}}') \\frac{\\partial \\phi({\\boldsymbol{r}}')}{\\partial n'} \\; {\\mathrm{d}}A' + \\langle \\phi \\rangle,\nwhere \\langle \\phi \\rangle = \\frac{1}{A} \\int_{\\partial \\Omega} \\phi({\\boldsymbol{r}}') \\; {\\mathrm{d}}A' is the average of \\phi over the surface. But remember from Theorem 3.1 that for Neumann boundary conditions the solution is defined up to a constant, so we can arbitrarily set the value \\langle \\phi \\rangle. Setting \\langle \\phi \\rangle = 0 is usually a very reasonable choice.\n\nYou may be wondering whether we can always find the Green’s functions for the Dirichlet and Neumann case. This requires solving the Laplace’s equation with the suitable boundary conditions to determine F({\\boldsymbol{r}}, {\\boldsymbol{r}}') in Equation 3.7. This is possible as long as \\partial \\Omega is “nice enough”.7 We may not be able to write such solutions explicitly, unfortunately, but even in these cases, writing the solution as in Proposition 3.3 is useful to understand its behaviour.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#method-of-images",
    "href": "3-applications-electrostatics.html#method-of-images",
    "title": "3  Applications of electrostatics",
    "section": "3.5 Method of images",
    "text": "3.5 Method of images\nIf Green’s functions allowed us to write down the solution for pretty much any boundary value problem, albeit usually not explicitly, the next method we will introduce is the opposite: it is a clever trick that will allow us to find explicit solutions to some simple (yet common and useful) charge configurations. This is called the method of images.\nLet’s go back to the discrete case: imagine we have a set of charges q_i at positions {\\boldsymbol{r}}_i \\in \\Omega, for i = 1 \\dots N. We can use superposition to compute the potential of the set from Equation 2.4, but this potential will (most likely) not satisfy the boundary conditions we would like to impose. The method of images consists on adding a set of virtual charges outside the domain, such that the combined effect of the actual and virtual charges satisfies the required boundary condition.\nThis is much more easily explained through an example.\n\nExample 3.4 (Charged particle near a conducting plane)  \n\n\n\n\n\n\nFigure 3.4: A grounded conductor on the left half-plane and a charge at (d,0,0). To simplify the diagram, we show the slice z=0.\n\n\n\nConsider a grounded conductor that fills the left half of the space, i.e. we have \\phi = 0 for x \\leq 0. We place a charge q at a point (d, 0, 0), and we want to know what is the induced electrostatic potential for x &gt; 0. The setup is shown in Figure 3.4.\nIf the conductor was not there, we already know that the potential would be\n\\phi = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\sqrt{(x - d)^2 + y^2 + z^2}},\nbut this clearly does not satisfy the boundary condition at x = 0 as\n\\phi = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\sqrt{d^2 + y^2 + z^2}} \\neq 0.\nThe clever trick is to assume the conductor is not there and, instead, place a charge -q at (-d, 0, 0). Then, the potential of the pair of charges is\n\\phi = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\sqrt{(x - d)^2 + y^2 + z^2}} -  \\frac{q}{\\sqrt{(x + d)^2 + y^2 + z^2}}\\right),\nand we can easily check that it satisfies \\phi=0 at x = 0. Now, of course this potential does not describe the actual potential of the conductor, but if we constrain ourselves to x &gt; 0 this potential works just fine. That has a quick solution though, just define a piecewise potential that is zero if x \\leq 0 and is the potential above otherwise.\nThis solution is extremely handy, and we can now use it to better understand how a charge behaves in the vicinity of a conductor. The first thing we can do is compute the electric field:\n{\\boldsymbol{E}}= - \\nabla \\phi = \\frac{q}{4 \\pi \\epsilon_0} \\left( \\frac{(x - d, y, z)}{\\left( (x-d)^2 + y^2 + z^2 \\right)^\\frac{3}{2}} - \\frac{(x + d, y, z)}{\\left( (x+d)^2 + y^2 + z^2 \\right)^\\frac{3}{2}} \\right), \\quad \\text{for} \\; x &gt; 0.\nNow let’s calculate the surface charge distribution. We need to use Proposition 3.2. As our surface is the y-z plane, the normal vector is {\\boldsymbol{n}}= {\\boldsymbol{e}}_x, so we only need the x-component of the electric field E_x. On the left of the boundary we have \\left. E^-_x \\right|_{x=0} = 0. On the right of the boundary we get from the expression above\n\\left. E^+_x \\right|_{x=0} = - \\frac{q}{2 \\pi \\epsilon_0} \\frac{d}{\\left( d^2 + y^2 + z^2 \\right)^\\frac{3}{2}}.\nTherefore, using Equation 3.2 we get\n\\sigma = \\epsilon_0 \\left. E^+_x \\right|_{x=0} = - \\frac{q}{2 \\pi} \\frac{d}{\\left( d^2 + y^2 + z^2 \\right)^\\frac{3}{2}},\nwhich is shown in Figure 3.5.\n\n\n\n\n\n\nFigure 3.5: Electric field generated by the point charge and the virtual charge used in the method of images.\n\n\n\nFinally, let’s compute the force that the charge will experience from the conductor. We can again use the potential induced by our virtual charge to write\n{\\boldsymbol{F}}= \\frac{-q^2}{16 \\pi \\epsilon_0 d^2} {\\boldsymbol{e}}_x.\nTherefore, the (actual) charge experiences an attractive force towards the conductor.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#sec-electric-dipole",
    "href": "3-applications-electrostatics.html#sec-electric-dipole",
    "title": "3  Applications of electrostatics",
    "section": "3.6 Electric dipoles",
    "text": "3.6 Electric dipoles\nLet’s now introduce the electric dipole, which is going to come extremely handy in Chapter 7 when we study how electric fields interact with matter. Consider two point charges with opposite sign, \\pm q, with the separation between them defined by a vector {\\boldsymbol{d}}. Now, let’s take the limit d = |{\\boldsymbol{d}}| \\to 0, while we let q \\to \\infty in such a way that the electric dipole moment {\\boldsymbol{p}}= q {\\boldsymbol{d}} remains constant. This is called an electric dipole.8 We can loosely interpret the dipole (and its moment) as a charge with an orientation.\nNow let’s compute the force that the dipole experiences from a given electric field {\\boldsymbol{E}}. We start with the pair of charges (i.e. before taking the limit d \\to 0). By the superposition principle the force is\n{\\boldsymbol{F}}= q {\\boldsymbol{E}}\\left({\\boldsymbol{r}}+ \\frac{{\\boldsymbol{d}}}{2}\\right) - q {\\boldsymbol{E}}\\left({\\boldsymbol{r}}- \\frac{{\\boldsymbol{d}}}{2}\\right),\nwhere, wlog, we have placed the charge +q at {\\boldsymbol{r}}+ \\frac{{\\boldsymbol{d}}}{2} and the charge -q at {\\boldsymbol{r}}- \\frac{{\\boldsymbol{d}}}{2}. Taking the Taylor expansion for small d we obtain\n\\begin{aligned}\n{\\boldsymbol{F}}&= q \\left( {\\boldsymbol{E}}({\\boldsymbol{r}}) + \\left(\\frac{{\\boldsymbol{d}}}{2} \\cdot \\nabla \\right) {\\boldsymbol{E}}+ O(d^2) \\right) - q \\left( {\\boldsymbol{E}}({\\boldsymbol{r}}) - \\left(\\frac{{\\boldsymbol{d}}}{2} \\cdot \\nabla \\right) {\\boldsymbol{E}}+ O(d^2) \\right)\\\\\n&= \\left( q {\\boldsymbol{d}}\\cdot \\nabla \\right) {\\boldsymbol{E}}({\\boldsymbol{r}}) + O(d^2),\n\\end{aligned}\n\nand when we take the limit d \\to 0 (assuming {\\boldsymbol{p}} stays constant) we obtain\n{\\boldsymbol{F}}= ({\\boldsymbol{p}}\\cdot \\nabla) {\\boldsymbol{E}}.\nAs a side note, we can also consider the torque that the field exerts on the dipole around the point {\\boldsymbol{r}}, which is\n {\\boldsymbol{\\tau}}= \\frac{{\\boldsymbol{d}}}{2} \\times \\left[ q {\\boldsymbol{E}}\\left({\\boldsymbol{r}}+ \\frac{{\\boldsymbol{d}}}{2}\\right) \\right] - \\frac{{\\boldsymbol{d}}}{2} \\times \\left[ -q {\\boldsymbol{E}}\\left({\\boldsymbol{r}}- \\frac{{\\boldsymbol{d}}}{2}\\right) \\right] \\to {\\boldsymbol{p}}\\times {\\boldsymbol{E}},\nwhere we have skipped the details for the Taylor expansion and the limit d \\to 0. Basically, this conveys the idea that the tendency of the dipoles will be to rotate until {\\boldsymbol{p}} and {\\boldsymbol{E}} are aligned. Interestingly if they point in the same direction we have a stable equilibrium, while if they point in opposite directions we have an unstable equilibrium. We omit the details here, but you can check it yourself thinking in which way does the torque operate when the angle between {\\boldsymbol{p}} and {\\boldsymbol{E}} is \\varepsilon \\ll 1 and \\pi - \\varepsilon.\nAn electric dipole also generates its own electric field. Similarly to how we constructed the force, we can conclude that the potential of a dipole with moment {\\boldsymbol{p}} placed at the origin is\n\\phi({\\boldsymbol{r}})_\\mathrm{dipole} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{{\\boldsymbol{p}}\\cdot {\\boldsymbol{r}}}{r^3}.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "3-applications-electrostatics.html#footnotes",
    "href": "3-applications-electrostatics.html#footnotes",
    "title": "3  Applications of electrostatics",
    "section": "",
    "text": "For simplicity, we constrain ourselves to the 2D case, e.g. take the z=0 plane.↩︎\nwlog = without loss of generality.↩︎\nThe conductor might also contain some immobile charges.↩︎\nThis is usually known as a Gaussian cylinder or Gaussian pillbox.↩︎\nTo formalise this argument, you would need to do an asymptotic analysis similar to what you learned in MA269 Asymptotics and Integral Transforms (if you took it).↩︎\nThis is an exercise in the problem sheets.↩︎\nIn this module it will be “nice enough” unless otherwise stated. For example, \\partial \\Omega being smooth is “nice enough”.↩︎\nThe concept of dipole might be familiar to you if you are taking MA3D1 Fluid Dynamics, as it appears in the potential flows chapter.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Applications of electrostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html",
    "href": "4-magnetostatics.html",
    "title": "4  Magnetostatics",
    "section": "",
    "text": "4.1 Electric current & conservation of charge\nSo far we have considered situations where charges are static, so the next step is to consider what happens when charges move around. However, when charges move around, they create something called magnetic fields! This can create some quite interesting (and messy) interactions between electric and magnetic fields. We will unpick this little by little starting with magnetostatics, that is the study of magnetic fields that do not depend on time.\nConsider a point charge. It’s pretty straightforward to picture this charge moving around the space with a velocity {\\boldsymbol{v}}(t). Let’s now think what would happen when we have infinitely many charges, so we consider a charge density \\rho({\\boldsymbol{r}}). Similarly, we can define a velocity field {\\boldsymbol{v}}({\\boldsymbol{r}}, t), which gives us the average1 drift velocity of a charge at point {\\boldsymbol{r}} and time t. In this chapter we focus on time independent problems, so we drop time, i.e. {\\boldsymbol{v}}({\\boldsymbol{r}}). This means that the charges can still move around, but the velocity does not change in time (though it can change in space). In short, the charge at a position {\\boldsymbol{r}} will always have the same velocity {\\boldsymbol{v}}({\\boldsymbol{r}}).\nIf you stop to consider what are the dimensions of the current density, you will find that they are charge per unit time and unit area.2 It is therefore a flux, that is (in very rigorous terms) amount of stuff through a surface per unit time, where in this case stuff is charge.\nIt is convenient to define electric current (or current, for short).\nThe physical interpretation of the current is the amount of charge per unit time flowing through the surface \\Sigma. Therefore, current is not a vector quantity but a scalar. Depending on which way we define the normal vector {\\boldsymbol{n}}, we may have positive or negative current. This arbitrarity is fine: we basically define a direction for current and, if it ends up being negative, it means charge is flowing in the opposite direction.\nYou are probably familiar with the idea of current when you think of electric cables. Charges inside might be doing all sorts of things, but what matters in the end is how much charge gets through the cross-section of the cable, i.e. the current.\nA very important property of electric charge is that it is conserved. Like with mass conservation, this means that charge cannot be created nor destroyed. The idea of conservation of stuff underpins many areas of physics3 and, luckily for us, we have a very convenient way of representing it mathematically.\nThe equation that encapsulates conservation of stuff is called the continuity equation. Even though it is a very general equation, here we will state it and derive it in the context of electric fields.4\nIn magnetostatics we have \\frac{\\partial \\rho}{\\partial t} = 0, therefore the continuity equation reduces to\n\\nabla \\cdot {\\boldsymbol{J}}= 0. \\tag{4.3}\nCurrents satisfying this property are called steady currents.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#electric-current-conservation-of-charge",
    "href": "4-magnetostatics.html#electric-current-conservation-of-charge",
    "title": "4  Magnetostatics",
    "section": "",
    "text": "Definition 4.1 (Current density) Given a distribution of charge with density \\rho and a velocity (vector) field {\\boldsymbol{v}}, the electric current density {\\boldsymbol{J}} is defined as\n {\\boldsymbol{J}}= \\rho {\\boldsymbol{v}}.\n\n\n\n\nDefinition 4.2 (Electric current) The electric current I through a surface \\Sigma is given by\nI = \\int_\\Sigma {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A.\nIn SI units, current is measured in amperes (A).\n\n\n\n\n\n\nTheorem 4.1 (Continuity equation) Given a charge density \\rho with a current density {\\boldsymbol{J}}, it must satisfy\n{\\frac{\\partial}{\\partial t}}{\\rho} + \\nabla \\cdot {\\boldsymbol{J}}= 0. \\tag{4.1}\n\n\nProof. Consider a bounded region \\Omega \\in {\\mathbb{R}}^3, bounded by the surface \\partial \\Omega. The total charge in \\Omega is given by\nQ = \\int_\\Omega \\rho \\; {\\mathrm{d}}V.\nBecause charge is conserved, the only way by which Q can change is either by charge coming in or charge coming out through the boundary \\partial \\Omega. This is defined by the current density {\\boldsymbol{J}}, so current through \\partial \\Omega is equal to minus the rate of change of Q:\n\\int_{\\partial \\Omega} {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = - \\frac{{\\mathrm{d}}Q}{{\\mathrm{d}}t} = - \\int_\\Omega \\frac{\\partial \\rho}{\\partial t} \\; {\\mathrm{d}}V. \\tag{4.2}\nThe minus sign is because we define the normal vector {\\boldsymbol{n}} to point outwards of \\Omega, therefore, if {\\boldsymbol{J}}\\cdot{\\boldsymbol{n}} is positive, it means charge is leaving \\Omega and therefore Q should decrease.\nApplying the divergence theorem to the left-hand side of Equation 4.2 we obtain\n \\int_\\Omega \\left( \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot {\\boldsymbol{J}}\\right) \\; {\\mathrm{d}}V = 0,\nand given that \\Omega is arbitrarily defined we must have\n \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot {\\boldsymbol{J}}= 0.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#lorentz-force-magnetic-field",
    "href": "4-magnetostatics.html#lorentz-force-magnetic-field",
    "title": "4  Magnetostatics",
    "section": "4.2 Lorentz force & magnetic field",
    "text": "4.2 Lorentz force & magnetic field\nWhen charges move around, they generate a magnetic field, which we will denote with {\\boldsymbol{B}}({\\boldsymbol{r}}, t) (though in this chapter we concert only with static problems, i.e. {\\boldsymbol{B}}({\\boldsymbol{r}})). The magnetic field, in turn, exerts a force over electric charges. In SI units the magnetic field is measure in teslas (T)5, which correspond to \\mathrm{N} \\; \\mathrm{s} \\; \\mathrm{m}^{-1} \\; \\mathrm{C}^{-1}.\n\nDefinition 4.3 (Lorentz force law) Given a point charge q under the effects of an electric field {\\boldsymbol{E}}({\\boldsymbol{r}}, t) and {\\boldsymbol{B}}({\\boldsymbol{r}}, t). The charge moves with a velocity {\\boldsymbol{u}}= \\frac{{\\mathrm{d}}{\\boldsymbol{r}}}{{\\mathrm{d}}t}, where {\\boldsymbol{r}}(t) is the position of the charge at a time t. Then, the charge experiences a force\n {\\boldsymbol{F}}({\\boldsymbol{r}}, t) = q {\\boldsymbol{E}}({\\boldsymbol{r}}, t) + q {\\boldsymbol{u}}(t) \\times {\\boldsymbol{B}}({\\boldsymbol{r}}, t).\n\nYou may have noticed we haven’t given a “proper” definition of the magnetic field. One may regard the Lorentz force law as the implicit definition of a magnetic field.\nNote that when the charge is static (i.e. {\\boldsymbol{u}}\\equiv {\\boldsymbol{0}}), the Lorentz force law gives\n {\\boldsymbol{F}}= q {\\boldsymbol{E}},\nas expected from electrostatics. Note as well that the component related to the magnetic field actually does no work on the particle. We can rewrite the definition of work (Equation 2.16) as\n W = - \\int_0^{t} {\\boldsymbol{F}}\\cdot {\\boldsymbol{u}}\\; {\\mathrm{d}}\\tilde t,\nwhich represents the work on the charge moving with velocity {\\boldsymbol{u}} during the time interval [0, t]. Then,\n W = - \\int_0^{t} q {\\boldsymbol{E}}\\cdot {\\boldsymbol{u}}\\; {\\mathrm{d}}\\tilde t,\nand the component related to the magnetic field vanishes because {\\boldsymbol{u}}(t) \\times {\\boldsymbol{B}}({\\boldsymbol{r}}, t) is, by the properties of the cross product, perpendicular to {\\boldsymbol{u}}(t). Therefore, its dot product with {\\boldsymbol{u}}(t) is equal to zero.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#biot-savart-law",
    "href": "4-magnetostatics.html#biot-savart-law",
    "title": "4  Magnetostatics",
    "section": "4.3 Biot-Savart law",
    "text": "4.3 Biot-Savart law\nIn the definition of the Lorentz force we have introduced the concept of magnetic field, but we have not yet given a mathematical definition like we did for the electric field Definition 2.3. This is the goal of this section.\nRemember that the electric field can be interpreted as the force that a set of static charges exerts on a unit test charge. Similarly, the magnetic field is the force that a set of moving charges exerts on a unit test charge. Its mathematical expression is given by the Biot-Savart law\n\nDefinition 4.4 (Biot-Savart law) A charge q at position {\\boldsymbol{r}}_0 and moving with a velocity {\\boldsymbol{v}} produces a magnetic field6\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0 q}{4 \\pi} \\frac{{\\boldsymbol{v}}\\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}_0)}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_0|^3},\nwhere \\mu_0 is a parameter called permeability of free space. Its value is \\mu_0 \\approx 1.257 \\times 10^{-6} \\; \\mathrm{N} \\; \\mathrm{A}^{-2}.\n\nNote the similarities between Definition 2.3 and Definition 4.4.\nThe Principle of Superposition also applies to magnetic fields. Therefore, the magnetic field generated by a steady current density {\\boldsymbol{J}} defined in a domain \\Omega is given by\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4\\pi} \\int_\\Omega \\frac{{\\boldsymbol{J}}({\\boldsymbol{r}}') \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} \\; {\\mathrm{d}}V'. \\tag{4.4}\nIn the particular case of a steady current I through a wire C we can write\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0 I}{4 \\pi} \\int_C \\frac{{\\mathrm{d}}{\\boldsymbol{r}}' \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3}, \\tag{4.5}\nwhere {\\mathrm{d}}{\\boldsymbol{r}}' is the differential of length along the curve C.\n\nExample 4.1 (Straight wire)  \n\n\n\n\n\n\nFigure 4.1: A current I through an infinitely long straight wire.\n\n\n\nLet’s consider a steady current I through an infinitely long straight wire. Wlog we will assume the wire is along the z axis (i.e. x=y=0), as shown in Figure 4.1. Then, we can work in cylindrical coordinates (\\rho, \\theta, z). In that case, the differential of length in the integral can be written as {\\mathrm{d}}{\\boldsymbol{r}}' = {\\boldsymbol{e}}_z {\\mathrm{d}}z. Then, we have\n {\\mathrm{d}}{\\boldsymbol{r}}' \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}') = \\rho \\; {\\mathrm{d}}z \\; {\\boldsymbol{e}}_\\theta,\ni.e. the magnetic field will point in the angular direction. Then, from Equation 4.5 we have\n {\\boldsymbol{B}}= \\frac{\\mu_0 I}{4 \\pi} {\\boldsymbol{e}}_\\theta \\int_{-\\infty}^{\\infty} \\frac{\\rho}{(\\rho^2 + z^2)^\\frac{3}{2}} \\; {\\mathrm{d}}z = \\frac{\\mu_0 I}{2 \\pi \\rho} {\\boldsymbol{e}}_\\theta.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#gauss-law-for-magnetism",
    "href": "4-magnetostatics.html#gauss-law-for-magnetism",
    "title": "4  Magnetostatics",
    "section": "4.4 Gauss’ law for magnetism",
    "text": "4.4 Gauss’ law for magnetism\nLet’s now derive Gauss’ law for magnetism which, as the name suggests, it is very similar to Gauss’ law for electric fields (Theorem 2.5).\n\nTheorem 4.2 (Gauss’ law for magnetism) Any magnetic field {\\boldsymbol{B}} satisfies\n\\nabla \\cdot {\\boldsymbol{B}}= 0.\n\n\nProof. We will prove Gauss’ law for magnetism for a point charge, but the proof for a current density would follow similarly, just with more complicated algebra. Wlog we set {\\boldsymbol{r}}_0 = {\\boldsymbol{0}}, so\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0 q}{4 \\pi} \\frac{{\\boldsymbol{v}}\\times {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3}\nFirst note that, using Equation 1.11, we get\n\\nabla \\cdot \\left( \\frac{{\\boldsymbol{v}}\\times {\\boldsymbol{r}}}{| {\\boldsymbol{r}}|^3} \\right) = \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} \\cdot \\left( \\nabla \\times {\\boldsymbol{v}}\\right) - {\\boldsymbol{v}}\\cdot \\left( \\nabla \\times \\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} \\right). \nAs {\\boldsymbol{v}} does not depend on {\\boldsymbol{r}} the first term vanishes. Now recall from Equation 1.14 that\n\\frac{{\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} = - \\nabla \\frac{1}{|{\\boldsymbol{r}}|},\nso the second term can be written as the curl of a gradient. Therefore, from Equation 1.7, we conclude\n\\nabla \\cdot \\left( \\frac{{\\boldsymbol{v}}\\times {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} \\right) = 0.\nThis shows that \\nabla \\cdot {\\boldsymbol{B}}= 0 everywhere except at {\\boldsymbol{r}}= {\\boldsymbol{0}} due to the singularity in the definition of {\\boldsymbol{B}}. To resolve it, we consider the ball of radius R centred at the origin, B_R. Then\n \\int_{\\partial B_R} {\\boldsymbol{B}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\frac{\\mu_0 q}{4 \\pi} \\int_{\\partial B_R} \\left( \\frac{{\\boldsymbol{v}}\\times {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3} \\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = 0,\nwhere in the final step we have used that {\\boldsymbol{v}}\\times {\\boldsymbol{r}} is perpendicular to the normal to the ball surface (which is in the radial direction). Then, by the divergence theorem, we find\n \\int_{B_R} \\nabla \\cdot {\\boldsymbol{B}}\\; {\\mathrm{d}}V = 0,\nand thus we conclude that \\nabla \\cdot {\\boldsymbol{B}}= 0 everywhere.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#ampères-law",
    "href": "4-magnetostatics.html#ampères-law",
    "title": "4  Magnetostatics",
    "section": "4.5 Ampère’s law",
    "text": "4.5 Ampère’s law\nSo far we have defined the magnetic field for a given charge and for a current in a wire. However, we would like a more generic equation that, given an electric current, tells us the magnetic field it induces. This is precisely what Ampère’s law7 does.\n\nTheorem 4.3 (Ampère’s law) The magnetic field {\\boldsymbol{B}} induced by a steady electric current density {\\boldsymbol{J}} satisfies\n\\nabla \\times {\\boldsymbol{B}}= \\mu_0 {\\boldsymbol{J}}.\nNote that this is equivalent to Equation 1.4 if the electric field {\\boldsymbol{E}} is steady.\n\nHistorically, this was derived by Ampère (and others) using experimental and theoretical tools. However, we can derive it from the Biot-Savart law (Equation 4.4). We need to introduce a couple of lemmas first.\n\nLemma 4.1 The vector field\n {\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4\\pi} \\int_\\Omega \\frac{{\\boldsymbol{J}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\; {\\mathrm{d}}V'  \\tag{4.6}\nsatisfies {\\boldsymbol{B}}({\\boldsymbol{r}}) = \\nabla \\times {\\boldsymbol{A}}({\\boldsymbol{r}}).\n\n\nProof. Let’s compute the derivative of the i-th component of {\\boldsymbol{A}} with respect to the j-th coordinate:\n\\frac{\\partial A_i}{\\partial x_j} = - \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{{\\boldsymbol{J}}_i({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} (x_j - x_j') {\\mathrm{d}}V',\nwhere we are using {\\boldsymbol{r}}= (x_1, x_2, x_3) and {\\boldsymbol{r}}' = (x'_1, x'_2, x'_3).\nFrom Definition 1.3 we have\n\\nabla \\times {\\boldsymbol{A}}= \\sum_{j=1}^3 {\\boldsymbol{e}}_j \\times \\frac{\\partial {\\boldsymbol{A}}}{\\partial x_j} = - \\sum_{i,j=1}^3 ({\\boldsymbol{e}}_i \\times {\\boldsymbol{e}}_j) \\frac{\\partial A_i}{\\partial x_j}.\nAnd we observe that, after some close manipulation, the last term is equal to {\\boldsymbol{B}}({\\boldsymbol{r}}) from Equation 4.4.\n\n\nLemma 4.2 For steady currents {\\boldsymbol{J}} supported inside a bounded region \\Omega we have \\nabla \\cdot {\\boldsymbol{A}}({\\boldsymbol{r}}) = 0.\n\n\nProof. First, note that if {\\boldsymbol{r}}\\neq {\\boldsymbol{r}}' we can write\n \\nabla \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|}\\right) = - \\nabla' \\left(\\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right),\nwhere \\nabla' is the gradient with respect to the {\\boldsymbol{r}}' variable.\nThen, from Equation 4.6\n\\begin{aligned}\n\\nabla \\cdot {\\boldsymbol{A}}&= \\frac{\\mu_0}{4 \\pi} \\int_\\Omega {\\boldsymbol{J}}({\\boldsymbol{r}}') \\cdot \\nabla \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) {\\mathrm{d}}V' \\\\\n&= -\\frac{\\mu_0}{4 \\pi} \\int_\\Omega {\\boldsymbol{J}}({\\boldsymbol{r}}') \\cdot \\nabla' \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) {\\mathrm{d}}V'\\\\\n&= -\\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\nabla' \\left( \\frac{{\\boldsymbol{J}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) {\\mathrm{d}}V' + \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\left(  \\nabla' \\cdot {\\boldsymbol{J}}({\\boldsymbol{r}}') \\right) {\\mathrm{d}}V'.\n\\end{aligned}\n\nThe last term vanishes as it satisfies Equation 4.3. For the remaining term, we can use divergence theorem:\n -\\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\nabla' \\left( \\frac{{\\boldsymbol{J}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) {\\mathrm{d}}V' = -\\frac{\\mu_0}{4 \\pi} \\int_{\\partial \\Omega} \\frac{{\\boldsymbol{J}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A'.\nBut, by definition, {\\boldsymbol{J}} is supported inside \\Omega, therefore the boundary integral vanishes and we conclude\n\\nabla \\cdot {\\boldsymbol{A}}= 0.\n\n\nProof. (of Ampère’s law)\nFrom Lemma 4.1 we have\n\\nabla \\times {\\boldsymbol{B}}= \\nabla \\times (\\nabla \\times {\\boldsymbol{A}}).\nUsing the identity Equation 1.9, which applies to any vector field {\\boldsymbol{A}}, we obtain\n \\nabla \\times (\\nabla \\times {\\boldsymbol{A}}) = \\nabla (\\nabla \\cdot {\\boldsymbol{A}}) - \\nabla^2 {\\boldsymbol{A}}.\nApplying Lemma 4.2 we deduce\n\\nabla \\times {\\boldsymbol{B}}= -\\nabla^2 {\\boldsymbol{A}}.\nNow we only need to compute \\nabla^2 {\\boldsymbol{A}} from Equation 4.6:\n\\nabla^2 {\\boldsymbol{A}}= \\frac{\\mu_0}{4\\pi} \\int_\\Omega {\\boldsymbol{J}}({\\boldsymbol{r}}') \\nabla^2 \\left(\\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|}\\right) {\\mathrm{d}}V' = \\frac{\\mu_0}{4\\pi} \\int_\\Omega {\\boldsymbol{J}}({\\boldsymbol{r}}') \\left(- 4 \\pi \\delta({\\boldsymbol{r}}- {\\boldsymbol{r}}') \\right) {\\mathrm{d}}V' = - \\mu_0 {\\boldsymbol{J}}({\\boldsymbol{r}}).\nWe have used Proposition 2.3 in the second step.\nCombining everything we conclude\n \\nabla \\times {\\boldsymbol{B}}= \\mu_0 {\\boldsymbol{J}}, \\tag{4.7}\nwhich is Ampère’s law (Theorem 4.3).\n\n\nCorollary 4.1 (Ampère’s law – alternative formulation) For any simple8 closed curve C = \\partial \\Sigma bounding a surface \\Sigma\n \\int_C {\\boldsymbol{B}}\\cdot {\\mathrm{d}}{\\boldsymbol{r}}= \\mu_0 \\int_\\Sigma {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\mu_0 I,\nwhere I is the current through \\Sigma.\n\n\nExample 4.2 (Straight wire – 2)  \n\n\n\n\n\n\nFigure 4.2: A current I through an infinitely long straight wire, but now we approach it by computing the integral along the curve C.\n\n\n\nLet’s consider again the current through a straight wire (Example 4.1), but now let’s approach it using Corollary 4.1. If we look at the symmetries of the problem, we notice it is invariant under vertical translations and rotations around the z-axis, so we can conclude that the magnetic field only depends on the radial direction. Let’s further assume that the only non-zero component is on the angular direction:\n {\\boldsymbol{B}}= B(\\rho) {\\boldsymbol{e}}_\\theta.\nFor our curve C we choose a circle of arbitrary radius \\rho in the \\rho-\\theta plane with the centre on the z axis. Corollary 4.1 gives\n \\int_C {\\boldsymbol{B}}\\cdot {\\mathrm{d}}{\\boldsymbol{r}}=  \\int_0^{2\\pi} B(\\rho) \\rho \\; {\\mathrm{d}}\\theta = 2 \\pi \\rho B(\\rho) = \\mu_0 I,\ntherefore we conclude that\n {\\boldsymbol{B}}= \\frac{\\mu_0 I}{2 \\pi \\rho} {\\boldsymbol{e}}_\\theta.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#magnetostatic-vector-potential",
    "href": "4-magnetostatics.html#magnetostatic-vector-potential",
    "title": "4  Magnetostatics",
    "section": "4.6 Magnetostatic vector potential",
    "text": "4.6 Magnetostatic vector potential\nIn the derivation of Ampère’s law we introduced the vector field {\\boldsymbol{A}}, purely for convenience. It turns out though, that there is a lot more to it.\n\nDefinition 4.5 (Magnetostatic vector potential) The vector potential of a static magnetic field {\\boldsymbol{B}} is defined as vector field {\\boldsymbol{A}} such that\n {\\boldsymbol{B}}= \\nabla \\times {\\boldsymbol{A}}.\n\nNote the parallelisms (and differences) with the electrostatic potential in Section 2.5. There, we had a scalar potential \\phi from which we computed the electric field as {\\boldsymbol{E}}= -\\nabla \\phi. Now, we have a vector potential9 {\\boldsymbol{A}} from which we compute the magnetic field {\\boldsymbol{B}}= \\nabla \\times {\\boldsymbol{A}}.\nNote as well that by defining the magnetic field from the vector potential it automatically satisfies \\nabla \\cdot {\\boldsymbol{B}}= 0, as the divergence of a curl is always zero (Equation 1.8). The implication works both ways and any field {\\boldsymbol{B}} that can be written as the curl of a potential satisfies \\nabla \\cdot {\\boldsymbol{B}}= 0.\nThe choice for {\\boldsymbol{A}} is far from unique, similarly to the definition of the electric potential. In this case, the argument is a bit more subtle, as we can not only add constants, but also vector fields of the form \\nabla \\chi (for a given function \\chi({\\boldsymbol{x}})):\n{\\boldsymbol{A}}' = {\\boldsymbol{A}}+ \\nabla \\chi \\implies \\nabla \\times {\\boldsymbol{A}}' = \\nabla \\times {\\boldsymbol{A}}+ \\nabla \\times (\\nabla \\chi) = \\nabla \\times {\\boldsymbol{A}}.\n\nDefinition 4.6 (Gauge transformation) The transformation\n {\\boldsymbol{A}}\\rightarrow {\\boldsymbol{A}}' = {\\boldsymbol{A}}+ \\nabla \\chi\nfor any given function \\chi is called a gauge transformation.\n\n\nProposition 4.1 We can always find a gauge transformation \\chi such that {\\boldsymbol{A}}' satisfies\n\\nabla \\cdot {\\boldsymbol{A}}' = 0.\nSuch transformation is called Coulomb gauge.\n\n\nProof. Suppose that we have a vector field {\\boldsymbol{A}} already satisfying \\nabla \\times {\\boldsymbol{A}}= {\\boldsymbol{B}}, but with \\nabla \\cdot {\\boldsymbol{A}}= \\psi({\\boldsymbol{x}}).\nConsider the transformed vector field {\\boldsymbol{A}}' = {\\boldsymbol{A}}+ \\nabla \\chi, taking the divergence we obtain\n\\nabla \\cdot {\\boldsymbol{A}}' = \\nabla \\cdot {\\boldsymbol{A}}+ \\nabla^2 \\chi = \\psi + \\nabla^2 \\chi.\nThen, to make \\nabla \\cdot {\\boldsymbol{A}}' = 0 we simply need to choose \\chi such that\n\\nabla^2 \\chi = - \\psi.\nThis is Poisson’s equation, and we know that it has a unique solution (up to an additive constant).\n\nThere is a lot to unpack from gauges when one goes into quantum physics (in fact, there is a class of quantum field theories called gauge theories). However, in this module we will use gauges purely as a mathematical tool: choosing the right gauge allows us to write equations in a simpler form. For example, Ampère’s law for the vector field yields\n\\mu_0 {\\boldsymbol{J}}= \\nabla \\times \\left( \\nabla \\times {\\boldsymbol{A}}\\right) = \\nabla (\\nabla \\cdot {\\boldsymbol{A}}) - \\nabla^2 {\\boldsymbol{A}},\nand taking the Coulomb gauge allows us to simplify the equation to\n \\nabla^2 {\\boldsymbol{A}}= - \\mu_0 {\\boldsymbol{J}},\nwhich is again Poisson’s equation that we already know how to solve.\n\nExample 4.3 (Straight wire – 3) Let’s consider yet again the current through a straight wire (Example 4.1). Now we want to compute the vector potential {\\boldsymbol{A}}. We found out in the previous examples that\n {\\boldsymbol{B}}= \\frac{\\mu_0 I}{2 \\pi \\rho} {\\boldsymbol{e}}_\\theta.\nAs {\\boldsymbol{B}}= \\nabla \\times {\\boldsymbol{A}}, we can write the following system of equations for the components of {\\boldsymbol{A}}:\n \\frac{1}{\\rho} \\frac{\\partial A_z}{\\partial \\theta} - \\frac{\\partial A_\\theta}{\\partial z} = 0,\n \\frac{\\partial A_\\rho}{\\partial z} - \\frac{\\partial A_z}{\\partial \\rho} = \\frac{\\mu_0 I}{2 \\pi \\rho},\n\\frac{1}{\\rho} \\left(\\frac{\\partial (\\rho A_\\theta)}{\\partial \\rho} - \\frac{\\partial A_\\rho}{\\partial \\theta} \\right) = 0.\nThis equation is not straightforward to solve, but we can construct a suitable solution (remember, it will not be unique) by taking\n{\\boldsymbol{A}}= - \\frac{\\mu_0 I}{2 \\pi} \\log \\left(  \\frac{\\rho}{\\rho_0} \\right) {\\boldsymbol{e}}_z,\nwhere \\rho_0 is an integrating constant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "4-magnetostatics.html#footnotes",
    "href": "4-magnetostatics.html#footnotes",
    "title": "4  Magnetostatics",
    "section": "",
    "text": "Real charges (e.g. electrons) will have some random motion, but they can still have an overall velocity, for example, induced by an electric field. We ignore (i.e. average out) the random motion and focus only on the overall drift velocity.↩︎\nIn SI units we have \\mathrm{A}\\; \\mathrm{m}^{-2}, where \\mathrm{A} = \\mathrm{C}\\;\\mathrm{s}^{-1} are amperes (name after Mr Ampère, who we will meet in a few pages).↩︎\nYou will encounter it in quite a few modules, e.g. MA3D1.↩︎\nThe generic form is derived in MA3D1.↩︎\nNamed after Nikola Tesla, not after a certain car brand…↩︎\nI am deliberately vague about whether the velocity can depend on time or not. It will be easier to discuss this when we reformulate the Biot-Savart law in terms of currents.↩︎\nNamed after André-Marie Ampère (1775-1836), a French physicist and mathematician.↩︎\ni.e. that it doesn’t intersect itself.↩︎\nIn some books you may encounter a magnetic scalar potential, which may confuse you. This scalar potential is used as a calculation technique in some very specific problems, and we will not use it in this module. If you want to learn more, there is a very good discussion in section 3.2.1 of David Tong’s book.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html",
    "href": "5-applications-magnetostatics.html",
    "title": "5  Applications of magnetostatics",
    "section": "",
    "text": "5.1 Surface currents\nHaving introduced the basic concepts for magnetostatics, let’s now put them into practice to better understand magnetism.\nIn Section 3.2 we talked about surface charges and their consequences on the continuity of the electric field. We now want to do a similar reasoning but for surface currents.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#surface-currents",
    "href": "5-applications-magnetostatics.html#surface-currents",
    "title": "5  Applications of magnetostatics",
    "section": "",
    "text": "Add section on surface currents",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#magnetic-monopoles",
    "href": "5-applications-magnetostatics.html#magnetic-monopoles",
    "title": "5  Applications of magnetostatics",
    "section": "5.2 Magnetic monopoles",
    "text": "5.2 Magnetic monopoles\nLet’s now consider more carefully the Gauss’ law for magnetic fields:\n\\nabla \\cdot {\\boldsymbol{B}}= 0.\nWhat this equation tells us, physically, is that there is no such thing as a magnetic point charge (more commonly known as a monopole). To see this, let’s compare it with Gauss’ law for electric fields:\n\\nabla \\cdot {\\boldsymbol{E}}= \\frac{\\rho}{\\epsilon_0}.\nIn this case, the point charges (or electric monopoles, to continue the parallelism) are the sources and sinks of electric field. Therefore, going back to the magnetic field equation, we conclude magnetic monopoles can’t exist.\nMathematically it might still make sense to allow for magnetic monopoles, and from a physical point of view they also are a useful tool to understand some effects (especially going down to quantum mechanics). However, no one has ever been able to observe a magnetic monopole in real life, so one should not worry very much about them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#sec-magnetic-dipoles",
    "href": "5-applications-magnetostatics.html#sec-magnetic-dipoles",
    "title": "5  Applications of magnetostatics",
    "section": "5.3 Magnetic dipoles",
    "text": "5.3 Magnetic dipoles\n\n\n\n\n\n\nFigure 5.1: Diagram of the magnetic field created by a current loop. Geek3, CC BY-SA 3.0, via Wikimedia Commons.\n\n\n\nNow let’s talk about magnetic dipoles. Consider the magnetic vector potential generated by a current I flowing through a small circle C, as shown in #fig-magnetic-dipole. From Equation 4.6, we have\n {\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0 I}{4 \\pi} \\int_C \\frac{{\\mathrm{d}}{\\boldsymbol{r}}'}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|}.\nWe now want to see what happens when we make this circle infinitely small. Wlog we take the C to be centred at the origin and lying on the (x,y)-plane. We can then parameterise it as {\\boldsymbol{r}}' = (R \\cos \\theta', R \\sin \\theta', 0) for \\theta' \\in [0, 2\\pi), where R &gt; 0 is the radius of the circle. If we take a Taylor expansion in the limit of R \\ll 1, we can write the integrand as\n \\begin{aligned}\n\\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} &= \\frac{1}{\\sqrt{(x - R \\cos \\theta')^2 + (y - R \\sin \\theta')^2 + z^2}} \\\\\n&= \\frac{1}{\\sqrt{x^2 + y^2 + z^2}} + R \\frac{x \\cos \\theta' + y \\sin \\theta'}{\\left(x^2 + y^2 + z^2 \\right)^\\frac{3}{2}} + O(R^2).\n\\end{aligned}\nUsing that {\\mathrm{d}}{\\boldsymbol{r}}' = (-R \\sin \\theta', R \\cos \\theta', 0) {\\mathrm{d}}\\theta', we can write the vector potential as\n\n\\begin{aligned}\n{\\boldsymbol{A}}({\\boldsymbol{r}}) &= \\frac{\\mu_0 I}{4 \\pi} \\int_C \\left(\\frac{1}{\\sqrt{(x - R \\cos \\theta')^2 + (y - R \\sin \\theta')^2 + z^2}} \\right) {\\mathrm{d}}{\\boldsymbol{r}}' \\\\\n&= \\frac{\\mu_0 I}{4 \\pi} \\int_0^{2\\pi} \\left[\\frac{1}{\\sqrt{x^2 + y^2 + z^2}} + R \\frac{x \\cos \\theta' + y \\sin \\theta'}{\\left(x^2 + y^2 + z^2 \\right)^\\frac{3}{2}} + O(R^2) \\right] (-R \\sin \\theta',R \\cos \\theta', 0) {\\mathrm{d}}\\theta'\\\\\n&= \\frac{\\mu_0 I}{4 \\pi r^3} \\left[\\pi R^2 (-y, x, 0) + O(R^2) \\right]\\\\\n&= \\frac{\\mu_0 I}{4 \\pi r^3} \\left[\\pi R^2 {\\boldsymbol{e}}_z \\times {\\boldsymbol{r}}+ O(R^2) \\right]\n\\end{aligned}\n\\tag{5.1}\nThe first term in the integral vanishes, as the integral is with respect to the tilde variables. Therefore, we are effectively taking the integral of a constant over a closed curve. In the last line we have just rewritten the result as a cross product for convenience.\nContinuing with rewriting things in a convenient way, let’s introduce the magnetic dipole moment:\n{\\boldsymbol{m}}= I \\pi R^2 {\\boldsymbol{n}},\nwhere {\\boldsymbol{n}} is the vector normal to the surface enclosed by C (in our calculations above we have {\\boldsymbol{n}}= {\\boldsymbol{e}}_z due to our choice of coordinates). Now let’s take the limit R \\to 0 while taking I \\to \\infty in such a way that I \\pi R^2 remains constant. Then, as the higher order terms vanish, the potential Equation 5.1 can be written as\n{\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\frac{{\\boldsymbol{m}}\\times {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3}. \\tag{5.2}\nCalculating the magnetic field of the dipole we obtain, after some algebra,\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\nabla \\times {\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\left(- \\frac{{\\boldsymbol{m}}}{|{\\boldsymbol{r}}|^3} + \\frac{3 ({\\boldsymbol{m}}\\cdot {\\boldsymbol{r}}) {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^5}\\right). \\tag{5.3}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#magnetic-forces",
    "href": "5-applications-magnetostatics.html#magnetic-forces",
    "title": "5  Applications of magnetostatics",
    "section": "5.4 Magnetic forces",
    "text": "5.4 Magnetic forces\nNow let’s turn our attention to magnetic forces. We will consider two cases: forces between currents and forces between dipoles.\n\n5.4.1 Force between currents\nWe first consider the force between two current distributions {\\boldsymbol{J}}_1({\\boldsymbol{r}}) and {\\boldsymbol{J}}_2 ({\\boldsymbol{r}}) defined in \\Omega_1 and \\Omega_2, respectively. Take the magnetic field produced by {\\boldsymbol{J}}_1, from the Biot-Savart law (Definition 4.4) we have\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\int_{\\Omega_1} \\frac{{\\boldsymbol{J}}_1({\\boldsymbol{r}}') \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} {\\mathrm{d}}V'.\nI we restrict ourselves to the case where we have a current I_1 through a curve (wire) C_1, we can write it as\n{\\boldsymbol{B}}({\\boldsymbol{r}}) = \\frac{\\mu_0 I_1}{4 \\pi} \\int_{C_1} \\frac{{\\mathrm{d}}{\\boldsymbol{r}}_1 \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}_1)}{|{\\boldsymbol{r}}-{\\boldsymbol{r}}_1|^3}.\nNow let’s bring in the current distribution {\\boldsymbol{J}}_2, which will experience a force due to {\\boldsymbol{J}}_1 of\n{\\boldsymbol{F}}=\\int_{\\Omega_2} {\\boldsymbol{J}}_2 ({\\boldsymbol{r}}) \\times {\\boldsymbol{B}}({\\boldsymbol{r}}) {\\mathrm{d}}V,\nor, if we again restrict to the case where we have a current I_2 through a curve C_2,\n{\\boldsymbol{F}}= I_2 \\int_{C_2} {\\mathrm{d}}{\\boldsymbol{r}}\\times {\\boldsymbol{B}}({\\boldsymbol{r}}).\nCombining the two results we obtain\n{\\boldsymbol{F}}= \\frac{\\mu_0}{4\\pi} I_1 I_2 \\int_{C_1} \\int_{C_2} {\\mathrm{d}}{\\boldsymbol{r}}_2 \\times \\left( {\\mathrm{d}}{\\boldsymbol{r}}_1 \\times \\frac{{\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1}{| {\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1|^3} \\right). \\tag{5.4}\nNotice the symmetry of the expression: if we were to compute the force on {\\boldsymbol{J}}_1 produced by {\\boldsymbol{J}}_2 we would reach the same result.\n\nExample 5.1 (Two straight wires) Consider the case were we have to infinite, straight, parallel wires a distance d from each other, carrying currents I_1 and I_2 respectively. We want the force applied to the second wire.\nWlog we can write {\\boldsymbol{r}}_1 = (0, 0, z_1) and {\\boldsymbol{r}}_2 = (d, 0, z_2), and derive that {\\mathrm{d}}{\\boldsymbol{r}}_1 = {\\mathrm{d}}z_1 \\; {\\boldsymbol{e}}_z and {\\mathrm{d}}{\\boldsymbol{r}}_2 = {\\mathrm{d}}z_2 \\; {\\boldsymbol{e}}_z. Then, Equation 5.4 becomes\n{\\boldsymbol{F}}= \\frac{\\mu_0}{4 \\pi} I_1 I_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} {\\boldsymbol{e}}_z \\times \\left({\\boldsymbol{e}}_z \\times \\frac{{\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1}{|{\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1|^3}\\right) {\\mathrm{d}}z_2 \\; {\\mathrm{d}}z_1.\nWe have {\\boldsymbol{r}}_2 - {\\boldsymbol{r}}_1 = d {\\boldsymbol{e}}_x + (z_2 - z_1) {\\mathrm{d}}{\\boldsymbol{e}}_z, and using the properties of the cross product we obtain\n\\begin{aligned}\n{\\boldsymbol{F}}&= \\frac{\\mu_0}{4 \\pi} I_1 I_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\frac{-d {\\boldsymbol{e}}_x}{\\left(d^2 + (z_2 - z_1)^2\\right)^\\frac{3}{2}} {\\mathrm{d}}z_2 {\\mathrm{d}}z_1\\\\\n&= - \\frac{\\mu_0}{2 \\pi d} I_1 I_2 {\\boldsymbol{e}}_x \\int_{-\\infty}^{\\infty} {\\mathrm{d}}z_2.\n\\end{aligned}\nThe second integral is infinite, which makes sense as it represents the length of the second wire. But if we consider only the force per unit length on the wire, we get\n{\\boldsymbol{f}}= - \\frac{\\mu_0}{2 \\pi d} I_1 I_2 {\\boldsymbol{e}}_x.\nIf the currents have the same sign (i.e. I_1 I_2 &gt; 0), then the force is attractive, while if the currents have opposite sign it is repulsive.\n\n\n\n5.4.2 Force between dipoles\nLet’s now calculate the force that a given magnetic field {\\boldsymbol{B}} exerts on a dipole. We will use the same argument that we used in the definition of the dipole (Section 5.3), in which we took the current around an infinitely small circular wire. From the Lorentz force law (Definition 4.3), the force on the dipole is\n{\\boldsymbol{F}}= \\int_C I {\\mathrm{d}}{\\boldsymbol{r}}' \\times {\\boldsymbol{B}}({\\boldsymbol{r}}').\nUsing the parameterisation {\\boldsymbol{r}}'=\\left(R \\cos \\theta', R \\sin \\theta', 0 \\right) for \\theta' \\in [0, 2\\pi), and taking the Taylor expansion in the limit of small R we get\n\\begin{aligned}\n{\\boldsymbol{F}}&= I \\int_0^{2\\pi} \\left(-R \\sin \\theta', R \\cos \\theta', 0 \\right) \\times \\left[ {\\boldsymbol{B}}({\\boldsymbol{0}}) + \\frac{\\partial}{\\partial x} {\\boldsymbol{B}}({\\boldsymbol{0}}) R \\cos \\theta' + \\frac{\\partial}{\\partial y} {\\boldsymbol{B}}({\\boldsymbol{0}}) R \\sin \\theta' + O(R^2) \\right] {\\mathrm{d}}\\theta' \\\\\n&= I \\pi R^2 \\left[ {\\boldsymbol{e}}_y \\times \\frac{\\partial}{\\partial x} {\\boldsymbol{B}}({\\boldsymbol{0}}) - {\\boldsymbol{e}}_x \\times \\frac{\\partial}{\\partial y} {\\boldsymbol{B}}({\\boldsymbol{0}}) + O(R^3) \\right] \\\\\n&= I \\pi R^2 \\left[ \\frac{\\partial}{\\partial x} B_z({\\boldsymbol{0}}) {\\boldsymbol{e}}_x + \\frac{\\partial}{\\partial y} B_z({\\boldsymbol{0}}) {\\boldsymbol{e}}_y - \\left( \\frac{\\partial}{\\partial x} B_x({\\boldsymbol{0}}) + \\frac{\\partial}{\\partial y} B_y({\\boldsymbol{0}}) + O(R^3) \\right)\\right] \\\\\n&= I \\pi R^2 \\left[\\nabla B_z({\\boldsymbol{0}}) - \\left( \\nabla \\cdot {\\boldsymbol{B}}({\\boldsymbol{0}}) \\right) {\\boldsymbol{e}}_z + O(R^3) \\right].\n\\end{aligned}\nTaking the limit R \\to 0 while keeping the dipole moment {\\boldsymbol{m}}= I \\pi R^2 {\\boldsymbol{e}}_z constant we obtain\n{\\boldsymbol{F}}= \\nabla ({\\boldsymbol{m}}\\cdot {\\boldsymbol{B}}) - (\\nabla \\cdot {\\boldsymbol{B}}) {\\boldsymbol{m}}= \\nabla({\\boldsymbol{m}}\\cdot {\\boldsymbol{B}}) = \\nabla ({\\boldsymbol{m}}\\cdot {\\boldsymbol{B}}), \\tag{5.5}\nwhere in the final step we have used that \\nabla \\cdot {\\boldsymbol{B}}= 0.\nThis force is conservative, and we can define the potential\nV_\\text{dipole} = - {\\boldsymbol{m}}\\cdot {\\boldsymbol{B}}, \\quad \\implies \\quad {\\boldsymbol{F}}= - \\nabla V_\\text{dipole}.\nThe torque around the origin (using Equation 1.6) is\n {\\boldsymbol{\\tau}}= \\int_C {\\boldsymbol{r}}' \\times \\left(I {\\mathrm{d}}{\\boldsymbol{r}}' \\times {\\boldsymbol{B}}({\\boldsymbol{r}}') \\right) = I \\int_C {\\mathrm{d}}{\\boldsymbol{r}}' \\left( {\\boldsymbol{r}}' \\cdot {\\boldsymbol{B}}({\\boldsymbol{r}}') \\right).\nBy a similar argument as before, we can write\n\\begin{aligned}\n{\\boldsymbol{\\tau}}&= I \\int_0^{2\\pi} \\left( -R \\sin \\theta' , R \\cos \\theta', 0 \\right) \\left[ B_x({\\boldsymbol{0}}) R \\cos \\theta' + B_y({\\boldsymbol{0}}) R \\sin \\theta' + O(R^2) \\right] {\\mathrm{d}}\\theta'\\\\\n&= I \\pi R^2 \\left[ (- B_y({\\boldsymbol{0}}), B_x({\\boldsymbol{0}}), 0) + O(R) \\right],\n\\end{aligned}\nand taking the limit R \\to 0 we obtain\n{\\boldsymbol{\\tau}}= {\\boldsymbol{m}}\\times {\\boldsymbol{B}}.\nNote that both for the force and the torque, the expressions are analogous to those of the electric dipole, but replacing {\\boldsymbol{p}} with {\\boldsymbol{m}} and {\\boldsymbol{E}} with {\\boldsymbol{B}}. This also implies that magnetic dipoles will try to align with the magnetic field.\n\nExample 5.2 (Force between dipoles) Now let’s consider the force between to dipoles with moment {\\boldsymbol{m}}_1 and {\\boldsymbol{m}}_2 separated by a distance {\\boldsymbol{d}} (with |{\\boldsymbol{d}}| = d). The magnetic field produced by the first dipole is given by Equation 5.3. Then, using Equation 5.5 we obtain that the force on the second dipole is\n{\\boldsymbol{F}}= \\frac{\\mu_0}{4\\pi} \\nabla \\left( - \\frac{{\\boldsymbol{m}}_1 \\cdot {\\boldsymbol{m}}_2}{d^3} + \\frac{3({\\boldsymbol{m}}_1 \\cdot {\\boldsymbol{d}})({\\boldsymbol{m}}_2 \\cdot {\\boldsymbol{d}})}{d^5} \\right).\nWe can compute the explicit expression (note that, here, the gradient applies on the vector {\\boldsymbol{d}}). After some careful1 manipulation, we obtain\n{\\boldsymbol{F}}= \\frac{3 \\mu_0}{4 \\pi d^4} \\left( ({\\boldsymbol{m}}_1 \\cdot \\hat{{\\boldsymbol{d}}}) {\\boldsymbol{m}}_2 + ({\\boldsymbol{m}}_2 \\cdot \\hat{{\\boldsymbol{d}}}) {\\boldsymbol{m}}_1 + ({\\boldsymbol{m}}_1 \\cdot {\\boldsymbol{m}}_2) \\hat{{\\boldsymbol{d}}} - 5 ({\\boldsymbol{m}}_1 \\cdot \\hat{{\\boldsymbol{d}}}) ({\\boldsymbol{m}}_2 \\cdot \\hat{{\\boldsymbol{d}}}) \\hat{{\\boldsymbol{d}}}  \\right),\nwhere \\hat{{\\boldsymbol{d}}} = {\\boldsymbol{d}}/ d is the unit vector pointing from {\\boldsymbol{m}}_1 to {\\boldsymbol{m}}_2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#application-permanent-magnets",
    "href": "5-applications-magnetostatics.html#application-permanent-magnets",
    "title": "5  Applications of magnetostatics",
    "section": "5.5 Application: permanent magnets",
    "text": "5.5 Application: permanent magnets\nAll the discussion we have had up to now was about magnetic fields arising from electric currents. But the magnets we are familiar with are the kind of thing we buy on our holidays to stick on our fridges. So where are the currents that produce the magnetic field in those magnets?\nThe answer to this question involves quantum mechanics. Even though quantum mechanics is out of scope for this model, let’s try to give some intuition about it.\n\n\n\n\n\n\nFigure 5.2: Diagram of the magnetic field produced by a bar (or straight) magnet. Left: iron fillings. Centre: field lines. Right: representation as compass needles. Geek3, CC BY-SA 4.0, via Wikimedia Commons.\n\n\n\nThe magnetism of such materials arises from the spin of the electrons. Electrons have an inherent angular momentum called spin. If you picture the electrons like tiny spheres you can imagine the spin in the same way as the Earth’s spin.2 It is this spin that produces a magnetic field. Typically, these magnetic fields point in all directions so their contributions basically cancel out at the macroscopic scale. However, when the spins of these many (many!) electrons align, the material produces a noticeable magnetic field.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "5-applications-magnetostatics.html#footnotes",
    "href": "5-applications-magnetostatics.html#footnotes",
    "title": "5  Applications of magnetostatics",
    "section": "",
    "text": "And painful!↩︎\nAs you probably know electrons are a lot more complicated in real life, so this analogy only will take us thus far.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of magnetostatics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html",
    "href": "6-electrodynamics.html",
    "title": "6  Electrodynamics",
    "section": "",
    "text": "6.1 Faraday’s Law of Induction\nUp to now, we only considered static situations. We could already infer from the stationary Maxwell’s equations that there are some connections between electric and magnetic fields: they are both produced by electric charges, the former by their existence, the latter by their motion.\nHowever, the interaction between electric and magnetic fields is much deeper, and this is only apparent when we let them vary in time. This is the focus of this chapter.\nLet’s first consider how Equation 2.15 is modified when we allow both {\\boldsymbol{E}} and {\\boldsymbol{B}} to change in time. This is given by Faraday’s law.1\nLet’s now introduce a couple of useful definitions.\nThen, Faraday’s law can be rewritten as\n\\mathcal{E}_\\mathrm{emf} = - \\frac{{\\mathrm{d}}\\Phi}{{\\mathrm{d}}t}.\nThis is simply a change in notation, but you may encounter this form in books.\nFaraday’s law tells us that if we change the magnetic flux through \\Sigma then we will induce a current. There are multiple ways of doing so physically. For example, one could change the magnetic field (e.g. by moving a magnet around) or maybe change the surface \\Sigma (e.g. by moving the wires enclosing it). There is another effect though. When a current flows in a wire, it will create its own magnetic field that will oppose the change that has induced the current in the first place. This is called Lenz’s law.\nWe can illustrate it as follows:\n\\text{change in } {\\boldsymbol{B}}\\xrightarrow[]{\\text{Faraday}} {\\boldsymbol{E}}\\xrightarrow[]{\\text{Lorentz}} \\text{current} \\xrightarrow[]{\\text{Ampère}} {\\boldsymbol{B}}.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#faradays-law-of-induction",
    "href": "6-electrodynamics.html#faradays-law-of-induction",
    "title": "6  Electrodynamics",
    "section": "",
    "text": "Theorem 6.1 (Faraday’s law) The relationship between the electric field {\\boldsymbol{E}} and the magnetic field {\\boldsymbol{B}} is given by\n \\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial {\\boldsymbol{B}}}{\\partial t} \n\n\nProof. This is not an actual proof, but we will show it for a particular case to get the idea. Generalising the proof is much trickier and out of the scope of this module.\nConsider the electric and magnetic fields generated by a set of charges moving with constant velocity {\\boldsymbol{v}} (i.e. moving along a straight line with constant speed). By the Lorentz force law (Definition 4.3), an observer that is not moving will measure a following force over a charge q:\n {\\boldsymbol{F}}= q {\\boldsymbol{E}}+ q {\\boldsymbol{v}}\\times {\\boldsymbol{B}}.\nNow consider an observer moving with the same constant velocity {\\boldsymbol{v}} as the charges. To that observer, the charges will look static and thus will only produce an electric field {\\boldsymbol{E}}'. They will measure a force over a charge q\n {\\boldsymbol{F}}' = q {\\boldsymbol{E}}'.\nNow since we assume the two observer should be measuring the same force2 we must have\n {\\boldsymbol{E}}' = {\\boldsymbol{E}}+ {\\boldsymbol{v}}\\times {\\boldsymbol{B}}. \nFor the second observer, the electric field is static, so Equation 2.15 applies. Then\n\n\\begin{aligned}\n{\\boldsymbol{0}}= \\nabla \\times {\\boldsymbol{E}}' &= \\nabla \\times {\\boldsymbol{E}}+ \\nabla \\times \\left( {\\boldsymbol{v}}\\times {\\boldsymbol{B}}\\right)\\\\\n&= \\nabla \\times {\\boldsymbol{E}}+ {\\boldsymbol{v}}(\\nabla \\cdot {\\boldsymbol{B}}) - ({\\boldsymbol{v}}\\cdot \\nabla) {\\boldsymbol{B}}\\\\\n&= \\nabla \\times {\\boldsymbol{E}}- ({\\boldsymbol{v}}\\cdot \\nabla) {\\boldsymbol{B}},\n\\end{aligned}\n\\tag{6.1}\nwhere in the second line we have used Equation 1.10 and in the third line we have applied Theorem 4.2.\nFor the first observer (the static one) all charges move with constant velocity {\\boldsymbol{v}}, so the magnetic field at position {\\boldsymbol{r}}+ {\\boldsymbol{v}}\\tau and time \\tau is the same as the magnetic field at position {\\boldsymbol{r}} and time t:\n{\\boldsymbol{B}}\\left({\\boldsymbol{r}}+ {\\boldsymbol{v}}\\tau, t + \\tau \\right) = {\\boldsymbol{B}}({\\boldsymbol{r}},t).\nThis is true for all \\tau, so we can divide by \\tau and take the limit \\tau \\to 0, obtaining3\n \\frac{\\partial {\\boldsymbol{B}}}{\\partial t} + \\left({\\boldsymbol{v}}\\cdot \\nabla \\right) {\\boldsymbol{B}}= {\\boldsymbol{0}}.\nSubstituting this back into Equation 6.1 we obtain\n \\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial {\\boldsymbol{B}}}{\\partial t}.\n\n\nCorollary 6.1 For any simple closed curve C = \\partial \\Sigma bounding a fixed surface \\Sigma, the Faraday law can be rewritten as\n \\int_C {\\boldsymbol{E}}\\cdot {\\mathrm{d}}{\\boldsymbol{r}}= - \\frac{{\\mathrm{d}}}{{\\mathrm{d}}t} \\int_\\Sigma {\\boldsymbol{B}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A.\n\n\n\nDefinition 6.1 (Magnetic flux) The integral \\Phi = \\int_\\Sigma {\\boldsymbol{B}}\\cdot {\\boldsymbol{n}}{\\mathrm{d}}A is called the magnetic flux through \\Sigma.\n\n\nDefinition 6.2 (Electromotive force (emf)) The integral \\mathcal{E}_\\mathrm{emf} = \\int_C {\\boldsymbol{E}}\\cdot {\\mathrm{d}}{\\boldsymbol{r}} is called the electromotive force.4",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#sec-displacement-current",
    "href": "6-electrodynamics.html#sec-displacement-current",
    "title": "6  Electrodynamics",
    "section": "6.2 Displacement current",
    "text": "6.2 Displacement current\nThe last step before being able to write the Maxwell’s equations is to revise Ampère’s law (Theorem 4.3) to account for time-dependent electric fields. To do this, let’s start with its integral form (Corollary 4.1):\n \\int_C {\\boldsymbol{B}}\\cdot {\\mathrm{d}}{\\boldsymbol{r}}= \\mu_0 \\int_\\Sigma {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A.\nRemember that C is a simple closed curve bounding \\Sigma. But there are infinitely many surfaces bounded by C, so let’s consider another surface \\Sigma' such that \\partial \\Sigma ' = C. Then\n0 = \\int_\\Sigma {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A - \\int_{\\Sigma'} {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_S {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A, \\tag{6.2}\nwhere S = \\Sigma \\cup \\Sigma' is a closed surface. This tells us that the flux of {\\boldsymbol{J}} through any closed surface must be zero. Another way of looking at this is to consider the volume \\Omega enclosed by S (i.e. \\partial \\Omega = S). Then, we can write\n\\int_S {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_\\Omega \\nabla \\cdot {\\boldsymbol{J}}\\; {\\mathrm{d}}V = 0,\nwhere we have applied the divergence theorem in the first equality and the steady current condition (Equation 4.3). But let’s now drop the static assumption. In that case, we need to use Equation 4.1 instead, so\n\\begin{aligned}\n\\int_S {\\boldsymbol{J}}\\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_\\Omega \\nabla \\cdot {\\boldsymbol{J}}\\; {\\mathrm{d}}V &= - \\int_\\Omega \\frac{\\partial \\rho}{\\partial t} \\; {\\mathrm{d}}V \\\\\n&= - \\epsilon_0 \\int_\\Omega \\frac{\\partial}{\\partial t} \\left( \\nabla \\cdot {\\boldsymbol{E}}\\right) \\;{\\mathrm{d}}V \\\\\n&= - \\epsilon_0 \\int_S \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A \\\\\n&= -\\epsilon_0 \\int_\\Sigma \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A + \\epsilon_0 \\int_{\\Sigma'} \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A,\n\\end{aligned}\n\nwhere we have used the continuity equation in the second line (Equation 4.1) and Gauss’ law (Equation 2.13) in the third line, before applying the divergence theorem again and splitting the surface integral over S back into \\Sigma and \\Sigma'.\nWe conclude that\n\\int_\\Sigma \\left( {\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A = \\int_{\\Sigma'} \\left( {\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\right) \\cdot {\\boldsymbol{n}}\\; {\\mathrm{d}}A,\nwhich suggests we can write Ampère’s law (Equation 4.7) as\n \\nabla \\times {\\boldsymbol{B}}= \\mu_0 \\left( {\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t}\\right). \\tag{6.3}\nThis is one of the Maxwell’s equation (Equation 1.4). This extra term that we picked up, \\frac{\\partial {\\boldsymbol{E}}}{\\partial t}, is called the displacement current.\nThere is a quite interesting historical note about this equation. While the other three Maxwell’s equations (and the static version of Ampère’s law) were discovered mostly through experiments (even though mathematicians were still needed to write them as equations), the displacement current term Equation 1.4 was discovered by Maxwell purely by reasoning, and that’s probably why all four equations are now named after him. Moreover, the displacement current must be included, otherwise the equations are not consistent, but we will discuss this in the next section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#maxwells-equations",
    "href": "6-electrodynamics.html#maxwells-equations",
    "title": "6  Electrodynamics",
    "section": "6.3 Maxwell’s equations",
    "text": "6.3 Maxwell’s equations\nSo we have finally derived Maxwell’s equations in their general form. We have Gauss’ law for electric fields\n\\nabla \\cdot {\\boldsymbol{E}}= \\frac{\\rho}{\\epsilon_0}, \\tag{6.4}\nand its equivalent for magnetic fields\n\\nabla \\cdot {\\boldsymbol{B}}= 0. \\tag{6.5}\nWe have not said anything about these equations in the time-dependent case, but it turns out they still hold (we just let {\\boldsymbol{E}}, {\\boldsymbol{B}} and \\rho depend on time).\nWe have now introduced Faraday’s law\n\\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial {\\boldsymbol{B}}}{\\partial t}, \\tag{6.6}\nand modified Ampère’s law to account for time-dependent fields\n\\nabla \\times {\\boldsymbol{B}}= \\mu_0 \\left({\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} \\right). \\tag{6.7}\nThese four equations, jointly with the Lorentz force law\n{\\boldsymbol{F}}= q \\left( {\\boldsymbol{E}}+ {\\boldsymbol{u}}\\times {\\boldsymbol{B}}\\right),\ndescribe the whole of electromagnetism.5\nLet’s take a closer look at the equations. The first two are scalar equations, while the latter two are vector equations. This means we have effectively 8 equations. Consider now the unknowns: each field has three components that we need to determine, so we have six unknowns in total. The system is overdetermined, so we must have two consistency conditions if we hope to get a solution.\nFirst let’s compute the time derivative of Equation 6.5,\n0 = \\frac{\\partial}{\\partial t} \\left( \\nabla \\cdot {\\boldsymbol{B}}\\right) = \\nabla \\cdot \\left(\\frac{\\partial {\\boldsymbol{B}}}{\\partial t} \\right) = - \\nabla \\cdot \\left( \\nabla \\times {\\boldsymbol{E}}\\right),\nwhich makes it consistent with Equation 6.6.\nThe second condition is probably a lot more interesting. Let’s now take the divergence of Equation 6.7, which yields\n0 = \\nabla \\cdot \\left( \\frac{1}{\\mu_0} \\nabla \\times {\\boldsymbol{B}}\\right) = \\nabla \\cdot {\\boldsymbol{J}}+ \\epsilon_0 \\frac{\\partial}{\\partial t} \\left( \\nabla \\cdot {\\boldsymbol{E}}\\right) = \\nabla \\cdot {\\boldsymbol{J}}+ \\frac{\\partial \\rho}{\\partial t},\nwhere in the last step we have used Equation 6.4. Therefore, the continuity equation is encapsulated in the Maxwell’s equations. This means that we can only get a solution to the Maxwell’s equation if \\rho and {\\boldsymbol{J}} satisfy the continuity equation (they would not be physical otherwise). This is why the displacement current is necessary in Equation 6.7.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#sec-light",
    "href": "6-electrodynamics.html#sec-light",
    "title": "6  Electrodynamics",
    "section": "6.4 Light",
    "text": "6.4 Light\nWe are now in a position to study what is probably one of the most astonishing outcomes of the Maxwell’s equations: the governing equation for light. The Maxwell’s equations tell us that the electric and magnetic fields are closely coupled together, and that oscillations on one affect the other.\nOur starting point will be the Maxwell’s equations in vacuum, that is assuming that there are no charges (i.e. \\rho = 0 and {\\boldsymbol{J}}= {\\boldsymbol{0}}). To derive the equations governing the behaviour of light, let’s start by taking the time derivative of Equation 6.7:\n \\mu_0 \\epsilon_0 \\frac{\\partial^2 {\\boldsymbol{E}}}{\\partial t^2} = \\frac{\\partial}{\\partial t} \\left( \\nabla \\times {\\boldsymbol{B}}\\right) = \\nabla \\times \\frac{\\partial {\\boldsymbol{B}}}{\\partial t} = - \\nabla \\times \\left( \\nabla \\times {\\boldsymbol{E}}\\right),\nwhere, remember, we are assuming {\\boldsymbol{J}}= {\\boldsymbol{0}}. Now let’s use the identity\n\\nabla \\times \\left( \\nabla \\times {\\boldsymbol{E}}\\right) = \\nabla \\left( \\nabla \\times {\\boldsymbol{E}}\\right) - \\nabla^2 {\\boldsymbol{E}}.\nBut by Equation 6.4 we conclude that the first term vanishes (remember, we assume \\rho = 0), so we can write\n\\frac{1}{c^2} \\frac{\\partial^2 {\\boldsymbol{E}}}{\\partial t^2} - \\nabla^2 {\\boldsymbol{E}}= {\\boldsymbol{0}},\nwhere we defined\nc = \\sqrt{\\frac{1}{\\mu_0 \\epsilon_0}}.\nSimilarly, we can take the time derivative of Equation 6.6 to obtain\n\\frac{1}{c^2} \\frac{\\partial^2 {\\boldsymbol{B}}}{\\partial t^2} - \\nabla^2 {\\boldsymbol{B}}= {\\boldsymbol{0}}.\nTherefore, both the electric and magnetic fields satisfy the wave equation.6 Let’s look at the parameter c we defined. Recall the values for the constants\n\\epsilon_0 \\approx 8.854 \\times 10^{-12} \\; \\mathrm{C}^2 \\; \\mathrm{N}^{-1} \\; \\mathrm{m}^{-2} \\quad \\text{and} \\quad \\mu_0 \\approx 1.257 \\times 10^{-6} \\; \\mathrm{N} \\; \\mathrm{A}^{-2},\nso the value of c is\nc \\approx 2.99 \\times 10^{8} \\; \\mathrm{m}\\;\\mathrm{s}^{-1},\nwhich is the speed of light!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#electromagnetic-potentials-and-gauge-transformations",
    "href": "6-electrodynamics.html#electromagnetic-potentials-and-gauge-transformations",
    "title": "6  Electrodynamics",
    "section": "6.5 Electromagnetic potentials and gauge transformations",
    "text": "6.5 Electromagnetic potentials and gauge transformations\nWe also need to think on how the introduction of the time dependency affects the electric and magnetic potentials we introduced in Definition 2.4 and Definition 4.5.\nSince Equation 6.5 still holds, we can define the magnetic potential {\\boldsymbol{A}}({\\boldsymbol{r}}, t) as\n {\\boldsymbol{B}}= \\nabla \\times {\\boldsymbol{A}}. \\tag{6.8}\nUsing Equation 6.6 we obtain\n0  = \\nabla \\times {\\boldsymbol{E}}+ \\frac{\\partial {\\boldsymbol{B}}}{\\partial t} = \\nabla \\times \\left( {\\boldsymbol{E}}+ \\frac{\\partial {\\boldsymbol{A}}}{\\partial t} \\right).\nTherefore, we can introduce a scalar potential \\phi({\\boldsymbol{r}}, t) such that\n{\\boldsymbol{E}}+ \\frac{\\partial {\\boldsymbol{A}}}{\\partial t} = - \\nabla \\phi.\nThen, the electric field can be in terms of both potentials as\n{\\boldsymbol{E}}= - \\nabla \\phi - \\frac{\\partial {\\boldsymbol{A}}}{\\partial t}. \\tag{6.9}\n\nDefinition 6.3 (Gauge transformation) The transformations\n {\\boldsymbol{A}}\\rightarrow {\\boldsymbol{A}}' = {\\boldsymbol{A}}+ \\nabla \\chi, \\quad \\text{and} \\quad \\phi \\rightarrow \\phi' = \\phi - \\frac{\\partial \\chi}{\\partial t} \nfor any given function \\chi are called a gauge transformations. These can be seen as a generalisation of Definition 4.6, and they leave Equation 6.8 and Equation 6.9 invariant.\n\n\nDefinition 6.4 (Lorenz gauge) The Lorenz7 gauge for {\\boldsymbol{A}} and \\phi is the condition\n\\frac{1}{c^2} \\frac{\\partial \\phi}{\\partial t} + \\nabla \\cdot {\\boldsymbol{A}}= 0.\n\nThis is analogous to the Coulomb gauge (Proposition 4.1), and it will allow us to choose a form for \\chi so we can write Equation 6.8 and Equation 6.9 in a more convenient form.\n\nTheorem 6.2 In Lorenz gauge, Maxwell’s equations can be written as\n\\frac{1}{c^2} \\frac{\\partial^2 \\phi}{\\partial t^2} - \\nabla^2 \\phi = \\frac{\\rho}{\\epsilon_0},\n\\frac{1}{c^2} \\frac{\\partial^2 {\\boldsymbol{A}}}{\\partial t^2} - \\nabla^2 {\\boldsymbol{A}}= \\mu_0 {\\boldsymbol{J}}.\n\n\nProof. Taking Equation 6.4 and plugging in Equation 6.9,\n \\frac{\\rho}{\\epsilon_0} = \\nabla \\cdot {\\boldsymbol{E}}= \\nabla \\cdot \\left( - \\nabla \\phi - \\frac{\\partial {\\boldsymbol{A}}}{\\partial t} \\right) = - \\nabla^2 \\phi + \\frac{1}{c^2} \\frac{\\partial^2 \\phi}{\\partial t},\nwhere in the last step we have used the Lorenz gauge.\nSimilarly, taking Equation 6.7 and substituting Equation 6.8, we obtain\n \\nabla \\times {\\boldsymbol{B}}= \\nabla \\left(\\nabla \\cdot {\\boldsymbol{A}}\\right) - \\nabla^2 {\\boldsymbol{A}}= \\mu_0 \\left[ {\\boldsymbol{J}}- \\epsilon_0 \\left( \\frac{\\partial}{\\partial t} \\nabla \\phi + \\frac{\\partial^2 {\\boldsymbol{A}}}{\\partial t^2} \\right) \\right].\nBut from Lorenz gauge we have\n\\nabla \\left( \\nabla \\cdot {\\boldsymbol{A}}\\right) = - \\epsilon_0 \\mu_0 \\frac{\\partial}{\\partial t} \\nabla \\phi,\ntherefore we can write\n-\\nabla^2 {\\boldsymbol{A}}+ \\frac{1}{c^2} \\frac{\\partial^2 {\\boldsymbol{A}}}{\\partial t^2} = \\mu_0 {\\boldsymbol{J}}.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#electromagnetic-energy-and-the-poynting-vector",
    "href": "6-electrodynamics.html#electromagnetic-energy-and-the-poynting-vector",
    "title": "6  Electrodynamics",
    "section": "6.6 Electromagnetic energy and the Poynting vector",
    "text": "6.6 Electromagnetic energy and the Poynting vector\nElectromagnetic waves carry energy. In fact, most of the energy we get in the Earth comes, in origin, from the light of the Sun. The aim of this section is to calculate this energy.\n\nDefinition 6.5 (Electromagnetic energy density) The electromagnetic energy density for a given field is\n\\mathcal{E} = \\frac{\\epsilon}{2}{\\boldsymbol{E}}\\cdot {\\boldsymbol{E}}+ \\frac{1}{2 \\mu_0} {\\boldsymbol{B}}\\cdot {\\boldsymbol{B}}.\n\nDo not confuse the electromagnetic energy density \\mathcal{E} with the electromotive force \\mathcal{E}_\\mathrm{emf}. In fact, the latter will not appear again in these notes.\n\nTheorem 6.3 (Poynting’s theorem) The electromagnetic energy density \\mathcal{E} satisfies\n\\frac{\\partial \\mathcal{E}}{\\partial t} + \\nabla \\cdot {\\boldsymbol{S}}= - {\\boldsymbol{E}}\\cdot {\\boldsymbol{J}}, \\tag{6.10}\nwhere\n{\\boldsymbol{S}}= \\frac{1}{\\mu_0} {\\boldsymbol{E}}\\times {\\boldsymbol{B}},\nis called the Poynting vector.\n\n\nProof. From Definition 6.5, taking the time derivative of the electromagnetic energy density we obtain\n\n\\begin{aligned}\n\\frac{\\partial \\mathcal{E}}{\\partial t} &= \\epsilon_0 {\\boldsymbol{E}}\\cdot \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} + \\frac{1}{\\mu_0} {\\boldsymbol{B}}\\cdot \\frac{\\partial {\\boldsymbol{B}}}{\\partial t}\\\\\n&= \\frac{1}{\\mu_0} {\\boldsymbol{E}}\\cdot \\left( \\nabla \\times {\\boldsymbol{B}}- \\mu_0 {\\boldsymbol{J}}\\right) - \\frac{1}{\\mu_0} {\\boldsymbol{B}}\\cdot \\left( \\nabla \\times {\\boldsymbol{E}}\\right) \\\\\n&= - \\nabla \\cdot \\left(\\frac{1}{\\mu_0} {\\boldsymbol{E}}\\times {\\boldsymbol{B}}\\right) - {\\boldsymbol{E}}\\cdot {\\boldsymbol{J}},\n\\end{aligned}\n\nwhere in the second line we have used Equation 6.6 and Equation 6.7, and the third line uses Equation 1.11.\n\nNote that that Equation 6.10 is analogous to Equation 4.1. Therefore, one can continue the analogy, and say that the Poynting vector {\\boldsymbol{S}} is to the electromagnetic energy density \\mathcal{E}, what the current density {\\boldsymbol{J}} is to the charge density \\rho. In other words, {\\boldsymbol{S}} is the flow of energy carried by the electromagnetic field. In the particular case where there are no currents, Equation 6.10 states that the electromagnetic energy is conserved. There is another interpretation8 of the Poynting vector though: it is the momentum stored in the electromagnetic field.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "6-electrodynamics.html#footnotes",
    "href": "6-electrodynamics.html#footnotes",
    "title": "6  Electrodynamics",
    "section": "",
    "text": "Named after Michael Faraday (1791-1867) the English chemist and physicist. He actually started the Royal Institution Christmas Lectures, which still carry on today. You can watch past lectures at the Royal Institution website.↩︎\nThings get trickier in the relativistic world.↩︎\nWe are omitting the details here, but this is the definition of the total derivation, widely used in physics, which account for the intrinsic change of {\\boldsymbol{B}} in time (first term) and the change in time due to the motion in space (second term).↩︎\nNot confusingly at all, the electromotive force is not actually a force. It is the tangential component of the force per unit charge integrated along C.↩︎\nAt least in the non-relativistic world.↩︎\nIf you are interested in learning more about waves, you should take MA301 Waves and Metamaterials.↩︎\nDo not confuse Mr Ludvig Lorenz (1829-1891) with Mr Hendrik Lorentz (1853-1928).↩︎\nWe will not elaborate on it in this module, though.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Electrodynamics</span>"
    ]
  },
  {
    "objectID": "7-matter.html",
    "href": "7-matter.html",
    "title": "7  Electromagnetism in matter",
    "section": "",
    "text": "7.1 Electric fields in matter\nIn all the preceding chapters we have considered the Maxwell’s equations in the vacuum. But in most real applications we want to consider, the electric and magnetic fields are not in the vacuum but interact with matter. The aim of this final chapter is to adjust Maxwell’s equations to account for the effects of matter.\nWe will start by considering electric fields in the presence of a matter, in particular a type of material called dielectric. Dielectric materials do not have any charges inside that can move around (they are all held in place).1 But dielectrics, like all matter, are made of atoms and, even though usually these atoms have neutral charge, its components don’t: the nucleus has positive charge and the cloud of electrons surrounding it has negative charge. This results in what we call polarisation,2 which means that when exposed to an electric field the nucleus gets slightly pushed in one directions and the electrons slightly pushed in the opposite direction. Polarisation has an effect on the electric properties of a material that we will need to take into account.\nRecall the concept of electric dipole that we introduced in Section 3.6. We can represent a dielectric medium as a collection of many (many) electric dipoles, as shown in Figure 7.1. Applying the principle of superposition, we can write the potential of the medium as\n\\phi({\\boldsymbol{r}})_\\mathrm{dipoles} = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^{N} \\frac{{\\boldsymbol{p}}_i \\cdot ({\\boldsymbol{r}}- {\\boldsymbol{r}}_i)}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|^3}.\nWith this definition, we can write the potential induced by a given polarisation density as\n\\phi({\\boldsymbol{r}})_\\mathrm{dipoles} = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{{\\boldsymbol{P}}({\\boldsymbol{r}}') \\cdot ({\\boldsymbol{r}}-{\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} {\\mathrm{d}}V'.\nNow, by a similar argument to the proof of Lemma 4.2, we can write\n\\begin{aligned}\n\\phi({\\boldsymbol{r}})_\\mathrm{dipoles} &= \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\nabla' \\cdot \\left(\\frac{{\\boldsymbol{P}}'({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) {\\mathrm{d}}V' - \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{\\nabla' \\cdot {\\boldsymbol{P}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V' \\\\\n&= \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{-\\nabla' \\cdot {\\boldsymbol{P}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V',\n\\end{aligned} \\tag{7.1}\nwhere in the last step we have used the divergence theorem on the first term, plus the fact that, by definition, {\\boldsymbol{P}} is zero at the boundary of \\Omega (and outside of it).\nComparing the equation above with Equation 2.11 we can see the convenience of defining\n\\rho_\\mathrm{bound}({\\boldsymbol{r}}) = - \\nabla \\cdot {\\boldsymbol{P}}({\\boldsymbol{r}}), \\tag{7.2}\nso we can rewrite Equation 7.1 as\n\\phi({\\boldsymbol{r}})_\\mathrm{dipoles} = \\frac{1}{4 \\pi \\epsilon_0} \\int_\\Omega \\frac{\\rho_\\mathrm{bound}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V'.\nThus we can interpret \\rho_\\mathrm{bound} as the effective charge density that produces the electric field Equation 7.1 caused by the dipole distribution. We also observe that Equation 7.2 looks like Gauss’ law but now with -{\\boldsymbol{P}}/\\epsilon_0 instead of {\\boldsymbol{E}}. So how is this related to Gauss’ law?\nWhen considering a material, we can define its total charge density \\rho (the one appearing in Gauss’ law) as the combination of the effective charge density \\rho_\\mathrm{bound} caused by the polarisation of the dielectric material, plus the free charge density \\rho_\\mathrm{free} (which is all the other charges that do not arise from polarisation). Then, Gauss’ law becomes\n\\nabla \\cdot {\\boldsymbol{E}}= \\frac{1}{\\epsilon_0} \\left( \\rho_\\mathrm{free} + \\rho_\\mathrm{bound} \\right) = \\frac{1}{\\epsilon_0} \\left( \\rho_\\mathrm{free} - \\nabla \\cdot {\\boldsymbol{P}}\\right).\nAs discussed earlier, we expect that the polarisation density {\\boldsymbol{P}} to be aligned with {\\boldsymbol{E}} everywhere, so we write\n\\frac{1}{\\epsilon_0} {\\boldsymbol{P}}= \\chi_e {\\boldsymbol{E}},\nwhere \\chi_e is a newly introduced parameter called the electric susceptibility of the dielectric material. It’s often more convenient to define \\chi_e in terms of the permittivity of the material \\epsilon = \\epsilon_0 (1 + \\chi_e), which quantifies the response of the dielectric material to an external electric field.3\nThe value of \\epsilon depends on many aspects of the material and the electric field, but in most cases we can take it to be approximately constant throughout the material. For vacuum, we have \\epsilon = \\epsilon_0, while for most other materials4 \\epsilon &gt; \\epsilon_0, e.g. for air \\epsilon \\approx \\epsilon_0 while for water \\epsilon \\approx 80 \\epsilon_0.\nUsing the permittivity, we can rewrite Gauss’ law as\n\\nabla \\cdot \\left( \\epsilon {\\boldsymbol{E}}\\right) = \\rho_\\mathrm{free},\nwhich sometimes is written in terms of the quantity {\\boldsymbol{D}}= \\epsilon {\\boldsymbol{E}}, called the electric displacement. Note that, as \\epsilon &gt; \\epsilon_0 the electric field generated by the free charges \\rho_\\mathrm{free} in a material will be smaller than that generated in the vacuum.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Electromagnetism in matter</span>"
    ]
  },
  {
    "objectID": "7-matter.html#electric-fields-in-matter",
    "href": "7-matter.html#electric-fields-in-matter",
    "title": "7  Electromagnetism in matter",
    "section": "",
    "text": "(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7.1: Diagrams for the dipole alignment for (a) unpolarised materials and (b) polarised materials. When the material is unpolarised the dipoles are randomly oriented. When we apply an electric field to the material (i.e. we polarise it), the dipoles align with the field.\n\n\n\n\n\n\nDefinition 7.1 (Electric polarisation density) We define the electric polarisation density as a vector field {\\boldsymbol{P}}({\\boldsymbol{r}}): \\Omega \\in \\mathbb{R}^3 \\to \\mathbb{R}^3 that gives the dipole moment per unit volume at a certain point in space. This is analogous to the electric charge density in Definition 2.6.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Electromagnetism in matter</span>"
    ]
  },
  {
    "objectID": "7-matter.html#magnetic-fields-in-matter",
    "href": "7-matter.html#magnetic-fields-in-matter",
    "title": "7  Electromagnetism in matter",
    "section": "7.2 Magnetic fields in matter",
    "text": "7.2 Magnetic fields in matter\nNow let’s look how matter affects magnetic fields. In the previous section we used electric dipoles to understand the behaviour of matter under an electric field. Now, we will follow a similar argument but with magnetic dipoles instead.\nRecall, from Equation 5.2, that the magnetic potential produced by a dipole is\n{\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\frac{{\\boldsymbol{m}}\\times {\\boldsymbol{r}}}{|{\\boldsymbol{r}}|^3}.\nThis is for a single dipole, but using the principle of superposition we can write\n{\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\sum_{i=1}^N \\frac{{\\boldsymbol{m}}_i \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}_i)}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}_i|^3}.\n\nDefinition 7.2 (Magnetisation density) We define the magnetisation density as a vector field {\\boldsymbol{M}}({\\boldsymbol{r}}): \\Omega \\in \\mathbb{R}^3 \\to \\mathbb{R}^3 that gives the magnetic dipole moment per unit volume at a certain point in space.\n\nThen, we can write the vector potential of a distribution of magnetic dipoles as\n{\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{{\\boldsymbol{M}}({\\boldsymbol{r}}') \\times ({\\boldsymbol{r}}- {\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|^3} {\\mathrm{d}}V'.\nNow, we can do some manipulation of this expression to obtain\n\\begin{aligned}\n{\\boldsymbol{A}}({\\boldsymbol{r}}) &= \\frac{\\mu_0}{4 \\pi} \\int_\\Omega {\\boldsymbol{M}}({\\boldsymbol{r}}') \\times \\nabla' \\left( \\frac{1}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right){\\mathrm{d}}V' \\\\\n&= \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\left[ \\frac{\\nabla' \\times {\\boldsymbol{M}}({\\boldsymbol{r}}') }{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} - \\nabla' \\times \\left( \\frac{{\\boldsymbol{M}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\right) \\right] {\\mathrm{d}}V' \\\\\n&= \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{\\nabla' \\times {\\boldsymbol{M}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V' + \\frac{\\mu_0}{4 \\pi} \\int_{\\partial \\Omega} \\frac{{\\boldsymbol{M}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} \\times {\\boldsymbol{n}}\\; {\\mathrm{d}}A' \\\\\n&= \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{\\nabla' \\times {\\boldsymbol{M}}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V'.\n\\end{aligned}\n\\tag{7.3}\nHere, in the second line we have used the product properties of the curl (Equation 1.12), in the third line we have applied Stokes’ theorem to the second term, and in the fourth line we have used the assumption that the magnetic dipole moment is zero at the boundary (similar argument to what we did with electric dipoles earlier). Now compare this expression with Equation 4.6, we may define\n {\\boldsymbol{J}}_\\text{bound}({\\boldsymbol{r}}) = \\nabla \\times {\\boldsymbol{M}}, \\tag{7.4}\nand thus rewrite Equation 7.3 as\n{\\boldsymbol{A}}({\\boldsymbol{r}}) = \\frac{\\mu_0}{4 \\pi} \\int_\\Omega \\frac{{\\boldsymbol{J}}_\\text{bound}({\\boldsymbol{r}}')}{|{\\boldsymbol{r}}- {\\boldsymbol{r}}'|} {\\mathrm{d}}V'.\nIn this case {\\boldsymbol{J}}_\\text{bound}({\\boldsymbol{r}}) represents the effective magnetising currents that generate the magnetic dipoles in the material. These current represents the added-up effect of all the microscopic currents induced by the magnetisation.5\nSimilarly with charge when we revisited Gauss’ law, the current appearing in Ampère’s law (Equation 4.7) is made of both the contributions due to the free charges {\\boldsymbol{J}}_\\text{free} and the contributions due to the effective magnetising current {\\boldsymbol{J}}_\\text{bound}. Then, Ampère’s law becomes\n \\nabla \\times {\\boldsymbol{B}}= \\mu_0 ({\\boldsymbol{J}}_\\text{free} + {\\boldsymbol{J}}_\\text{bound}) = \\mu_0 {\\boldsymbol{J}}_\\text{free} + \\mu_0 \\nabla \\times {\\boldsymbol{M}}.\nFor most materials, the magnetisation density will align with the magnetic field {\\boldsymbol{B}} such that there is no torque on the dipoles (same argument as with electric dipoles), therefore it is reasonable to write\n {\\boldsymbol{M}}= \\frac{1}{\\mu_0} \\frac{\\chi_m}{1 + \\chi_m} {\\boldsymbol{B}},\nwhere \\chi_m (you probably see it coming) is a newly introduced parameter called the magnetic susceptibility of the material. You probably won’t be surprised either that we may want to define the permeability6 of the material \\mu = \\mu_0 (1 + \\chi_m).\nThen, we can write\n{\\boldsymbol{M}}= \\frac{\\chi_m}{\\mu} {\\boldsymbol{B}}, \\tag{7.5}\nand therefore Ampère’s law can be written as\n\\nabla \\times \\left( \\frac{1}{\\mu} {\\boldsymbol{B}}\\right) = {\\boldsymbol{J}}_\\text{free}, \\tag{7.6}\nwhich sometimes is written in terms of the quantity {\\boldsymbol{H}}= \\frac{1}{\\mu}{\\boldsymbol{B}}, called the magnetising field.\nBefore finishing the section, let’s have a quick discussion about what values can \\chi_m take:\n\nIf -1 &lt; \\chi_m &lt; 0 the material is called diamagnetic. This means that the magnetisation of the material opposes the applied magnetic field (i.e. the object is repelled by the magnetic field). Some examples of diamagnetic materials are copper, gold and water.7\nIf \\chi_m &gt; 0 the material is called paramagnetic. This means that the magnetisation of the material points in the same direction of the applied magnetic field (i.e. the object is attracted by the magnetic field). Some examples of paramagnetic materials are aluminium or tungsten.\nIn some cases, we can have {\\boldsymbol{M}}\\neq {\\boldsymbol{0}} when {\\boldsymbol{B}}= {\\boldsymbol{0}}, so Equation 7.5 doesn’t hold. These are known as ferromagnetic materials. These materials become magnetised very easily by external magnetic fields and, more importantly, remain magnetised long after the field is gone. These are the materials that we commonly know as magnets. Very few elements are ferromagnetic, most notably iron, nickel and cobalt.\n\nThere is one final aspect we need to discuss. Equation 7.6 holds for the magnetostatic case, but similarly to what we did in Section 6.2 we need to modify it to account for time-dependent fields.\nWhen the fields are time-dependent, the bound charge \\rho_\\text{bound} no longer sits still but it moves around. Still, it must satisfy the continuity equation Equation 4.1, i.e.\n{\\frac{\\partial}{\\partial t}}{\\rho_\\text{bound}} + \\nabla \\cdot {\\boldsymbol{J}}_\\text{bound} = 0.\nRecall, from Equation 7.2, that \\rho_\\text{bound} is related to the polarisation density, so we can rewrite Equation 7.4 as\n{\\boldsymbol{J}}_\\text{bound} = \\nabla \\times {\\boldsymbol{M}}+ \\frac{\\partial {\\boldsymbol{P}}}{\\partial t}.\nNow we can use a similar argument as before, but starting from the time-dependent Ampère’s law Equation 6.3 we have\n\\nabla \\times {\\boldsymbol{B}}- \\mu_0 \\epsilon_0 \\frac{\\partial {\\boldsymbol{E}}}{\\partial t} = \\mu_0 \\left( {\\boldsymbol{J}}_\\text{free} + {\\boldsymbol{J}}_\\text{bound} \\right) = \\mu_0 {\\boldsymbol{J}}_\\text{free} + \\mu_0 \\nabla \\times {\\boldsymbol{M}}+ \\mu_0 \\frac{\\partial {\\boldsymbol{P}}}{\\partial t}.\nRearranging it in terms of {\\boldsymbol{H}} and {\\boldsymbol{D}} we obtain\n \\nabla \\times {\\boldsymbol{H}}- \\frac{\\partial {\\boldsymbol{D}}}{\\partial t} = {\\boldsymbol{J}}_\\text{free}.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Electromagnetism in matter</span>"
    ]
  },
  {
    "objectID": "7-matter.html#macroscopic-maxwells-equations",
    "href": "7-matter.html#macroscopic-maxwells-equations",
    "title": "7  Electromagnetism in matter",
    "section": "7.3 Macroscopic Maxwell’s equations",
    "text": "7.3 Macroscopic Maxwell’s equations\nNow we can put together the results of the previous sections into the macroscopic Maxwell’s equations. Inside matter, electromagnetic fields are governed by\n\\nabla \\cdot {\\boldsymbol{D}}= \\rho_\\text{free}, \\tag{7.7} \\nabla \\cdot {\\boldsymbol{B}}= 0, \\tag{7.8} \\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial{\\boldsymbol{B}}}{\\partial t}, \\tag{7.9} \\nabla \\times {\\boldsymbol{H}}- \\frac{\\partial {\\boldsymbol{D}}}{\\partial t} = {\\boldsymbol{J}}_\\text{free}. \\tag{7.10}\nNote that two of the equations are written using {\\boldsymbol{E}} and {\\boldsymbol{B}}, while the other two use {\\boldsymbol{D}} and {\\boldsymbol{H}}. Therefore, we need some additional constraints to relate these quantities. However, we have seen that in the simplest case we can write\n{\\boldsymbol{D}}= \\epsilon {\\boldsymbol{E}}, \\quad \\text{and} \\quad {\\boldsymbol{B}}= \\mu {\\boldsymbol{H}}.\nMaterials that behave like this are callen linear materials, and all the complexity of the material is absorbed into the permittivity \\epsilon and the permeability \\mu, which we assume to be constant. Things are a bit more complicated in real life, and many materials do not behave linearly, but that is out of the scope of this module.8\n\n7.3.1 Waves in matter\nLet’s study now how electromagnetic waves propagate through matter. As we did in Chapter 6 for vacuum, we will constrain to the case where there is no free charge nor current, and we will study only linear materials. Then, Maxwell’s equations simplify to\n\\nabla \\cdot \\left(\\epsilon {\\boldsymbol{E}}\\right) = 0, \\tag{7.11} \\nabla \\cdot {\\boldsymbol{B}}= 0, \\tag{7.12} \\nabla \\times {\\boldsymbol{E}}= - \\frac{\\partial{\\boldsymbol{B}}}{\\partial t}, \\tag{7.13} \\nabla \\times \\left( \\frac{{\\boldsymbol{B}}}{\\mu}\\right) = \\frac{\\partial \\left(\\epsilon {\\boldsymbol{E}}\\right)}{\\partial t}. \\tag{7.14}\nApplying the same transformations as in Section 6.4, we can derive the wave equations\n\\frac{1}{v^2} \\frac{\\partial^2 {\\boldsymbol{E}}}{\\partial t^2} - \\nabla^2 {\\boldsymbol{E}}= {\\boldsymbol{0}},\nand\n\\frac{1}{v^2} \\frac{\\partial^2 {\\boldsymbol{B}}}{\\partial t^2} - \\nabla^2 {\\boldsymbol{B}}= {\\boldsymbol{0}},\nwhere the new wave propagation speed is given by v = (\\epsilon \\mu)^{-\\frac{1}{2}}. It’s not directly obvious from the definitions of \\epsilon and \\mu, but for all materials v \\leq c, so matter basically slows light down. There are also other interesting effects that we will briefly discuss later in this chapter. Note that as the relation between variables is linear, we could replace {\\boldsymbol{E}} by {\\boldsymbol{D}} and {\\boldsymbol{B}} by {\\boldsymbol{H}} and the equations would still hold.\n\n\n7.3.2 Boundary conditions\nTo conclude this section, we need to talk about what happens at the interface between two dielectric materials, with different permitivities and permeabilities. We already showed in Chapter 3 and Chapter 5 that at a surface, electric fields are continuous in the tangent direction but may be discontinuous in the normal direction, and viceversa for magnetic fields. The culprits for those discontinuities were surface charges and currents.\nWe now want to extend these conditions for electromagnetic fields in matter. To do so, we will repeat the arguments from previous sections, but we will apply them to Equation 7.7 – Equation 7.10. We label the two materials 1 and 2, and the normal vector {\\boldsymbol{n}} is defined to point from material 1 to material 2.\nWe first apply the pillbox argument to Equation 7.7, and in the presence of a (free) surface charge \\sigma we obtain\n{\\boldsymbol{n}}\\cdot \\left({\\boldsymbol{D}}_2 - {\\boldsymbol{D}}_1\\right) = \\sigma.\nSimilarly, from Equation 7.8 we obtain\n{\\boldsymbol{n}}\\cdot \\left({\\boldsymbol{B}}_2 - {\\boldsymbol{B}}_1\\right) = 0.\nNow we need to integrate along a close curve that intersects the boundary. From Equation 7.9 we obtain\n{\\boldsymbol{n}}\\times \\left({\\boldsymbol{E}}_2 - {\\boldsymbol{E}}_1 \\right) = {\\boldsymbol{0}},\nwhile for Equation 7.10 we obtain\n{\\boldsymbol{n}}\\times \\left({\\boldsymbol{H}}_2 - {\\boldsymbol{H}}_1 \\right) = {\\boldsymbol{K}},\nwhere {\\boldsymbol{K}} is the surface current.\nNote that we have obtain one condition per field {\\boldsymbol{E}}, {\\boldsymbol{D}}, {\\boldsymbol{B}} and {\\boldsymbol{H}}. Typically we will only use two of them, but we can convert between them using the relevant constraints for the materials we are considering.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Electromagnetism in matter</span>"
    ]
  },
  {
    "objectID": "7-matter.html#footnotes",
    "href": "7-matter.html#footnotes",
    "title": "7  Electromagnetism in matter",
    "section": "",
    "text": "Dielectric materials are similar to insulators, which we saw earlier on, but they are not quite the same thing.↩︎\nThis is different from light polarisation.↩︎\nNow you see why we called \\epsilon_0 the permittivity of free space.↩︎\nThe exceptions are very weird materials in very weird situations, so unless stated otherwise it is safe to assume this is true.↩︎\nThere is a more detailed version of this argument in Chapter 8.2.1 of Tong’s book, and here is a cute animation in Wikipedia that illustrates it.↩︎\nDon’t mix it up with the permittivity!↩︎\nTherefore humans are also diamagnetic, as we are made mostly of water.↩︎\nFerromagnetic materials are a clear example of materials that do not behave linearly.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Electromagnetism in matter</span>"
    ]
  }
]